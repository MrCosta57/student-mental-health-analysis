---
title: "Mind Matters: A Data-driven Examination of College Students' Mental Health"
subtitle: "Statistical Inferecence and Learining project"
author: "Giovanni Costa - 880892"
date: "AY 2023/24"
geometry: "left=1cm,right=2cm,top=1cm,bottom=1cm"
output:
  html_document:
    toc: true
    number_sections: true
    toc_depth: 2
    toc_float:
      smooth_scroll: false
    fig_caption: yes
    theme: flatly
    highlight: pygments
    css: "assets/css/styles.css"
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction
This project aims to delve into the pressing issue of mental health among college students, a topic of significant importance in today's fast-paced, high-stress academic environment. The analysis seeks to provide a comprehensive understanding of the mental health landscape, by utilizing one dataset coming from a survey, with a particular focus on psychiatric symptoms and other associated health problems.

The importance of this analysis lies in its potential to uncover hidden patterns and relationships among various factors affecting students' mental health in provided data, to better understand their contributions in this issue.

One of the key purposes of this project is to predict whether certain individuals may be at a higher risk of suicidal ideation. This predictive analysis could potentially serve as an early warning system, enabling universities to provide timely intervention and support to those in need.

Furthermore, this project aims to provide actionable insights that can guide the development of more effective mental health policies and support systems within universities. By understanding the specific challenges faced by students, universities can tailor their resources and initiatives to better address these issues, ultimately fostering a healthier, more supportive academic environment.

The dataset, that can be retrieved <a href="https://psycharchives.org/en/item/5d25cf8a-7910-4579-9e39-688c6bbec43a">here</a>, contains 27014 objects with 16 features. The first six are categorical information about the general individual condition, instead the others are numeric variables related to the mental health of the students, where the higher score indicates more serious symptoms. Additional details regarding the data scales, where the data comes from and the methodology for retrieving them are not provided by the dataset website.<br/>
A specific description of these features is provided below:

1. **gender**: 0=Female, 1=Male
2. **whether_only_child**: 0=No, 1=Yes
3. **birth_place**: 0=Countryside, 1=Town, 2=SmallCity, 3=MediumToLargeCities
4. **family_economic_status**: 0=ExtremelyPoor, 1=Poor, 2=Average, 3=Good, 4=Rich
5. **major**: 0=Liberal, 1=Science, 2=Art
6. **grade**: 0=Postgraduate, 1=UndergraduateGradeFive, 2=Junior, 3=Sophomore, 4=Freshman, 5=Senior
7. **psychiatric_symptoms**: range from 4 to 16
8. **suicide**: range from 4 to 16
9. **dependence**: range from 4 to 16
10. **impulsivity**: range from 4 to 16
11. **compulsion**: range from 4 to 16
12. **sleeping_disturbance**: range from 4 to 16
13. **internet_addiction**: range from 5 to 20
14. **hostile_aggression**: range from 4 to 16
15. **self_injury_behaviors:** range from 4 to 16
16. **eating_problems**: range from 4 to 16

The purposes of this analysis are inspecting the possible hidden patterns present in the data and predicting the risk of observing a suicide, modeled for convenience as a binary variable. 

# Prerequirements
```{r results='FALSE', message=FALSE, warning=FALSE}
set.seed(123) # set pseudorandom generator for reprocucibility

# Load all the packages and install them if they are not present
requirements <- c(
  "summarytools", "MASS", "effects", "pROC", "mgcv",
  "glmnetUtils", "e1071", "class",
  "stringr", "ggplot2", "reshape2", "scales", "comprehenr",
  "randomForest", "dplyr", "ggcorrplot"
)

for (library_name in requirements) {
  if (!require(library_name, character.only = TRUE)) {
    install.packages(library_name, repos = "https://cloud.r-project.org")
    library(library_name, character.only = TRUE)
  }
}
# Import user defined functions
source("utils.R")
# library(styler)
# style_file("analysis_report.Rmd")
```


# First overview and data engineering

In this section, exploratory data analysis (EDA) is performed to understand the structure of the dataset and identify any potential issues that need to be addressed. Also, data engineering is applied to transform the variables into the appropriate format.
```{r}
# Read the dataset
df <- read.csv("data/mental_health_data.csv")
# Remove the column representing the row index
df <- df[, -1]

# Rename existing columns
colnames(df)[colnames(df) == "whether.only.child"] <- "whether_only_child"
colnames(df)[colnames(df) == "birth.place"] <- "birth_place"
colnames(df)[colnames(df) == "family.economic.status"] <- "family_economic_status"
colnames(df)[colnames(df) == "psychiatric.symptoms"] <- "psychiatric_symptoms"
colnames(df)[colnames(df) == "sleeping.disturbance"] <- "sleeping_disturbance"
colnames(df)[colnames(df) == "internet.addiction"] <- "internet_addiction"
colnames(df)[colnames(df) == "hostile.aggression"] <- "hostile_aggression"
colnames(df)[colnames(df) == "self.injury.behaviors"] <- "self_injury_behaviors"
colnames(df)[colnames(df) == "eating.problems"] <- "eating_problems"


# Transforming variables into factors
df$gender <- factor(df$gender,
  levels = c(0, 1),
  labels = c("Female", "Male")
)
df$whether_only_child <- factor(df$whether_only_child,
  levels = c(0, 1),
  labels = c("No", "Yes")
)
df$birth_place <- factor(
  df$birth_place,
  levels = 0:3,
  labels = c("Countryside", "Town", "SmallCity", "MediumToLargeCities")
)
df$family_economic_status <- factor(
  df$family_economic_status,
  levels = 0:4,
  labels = c("ExtremelyPoor", "Poor", "Average", "Good", "Rich")
)
df$major <- factor(df$major,
  levels = 0:2,
  labels = c("Liberal", "Science", "Art")
)
df$grade <- factor(
  df$grade,
  levels = 0:5,
  labels = c(
    "Postgraduate",
    "UndergraduateGradeFive",
    "Junior",
    "Sophomore",
    "Freshman",
    "Senior"
  )
)

general_cols <- c("gender", "whether_only_child", "birth_place", "family_economic_status", "major", "grade")
symptoms_cols <- c("psychiatric_symptoms", "dependence", "impulsivity", "compulsion", "sleeping_disturbance", "internet_addiction", "hostile_aggression", "self_injury_behaviors", "eating_problems")
```

```{r}
print_summary_custom(df)
```

```{r}
# "Dataset dimensions:"
dim(df)
# "N. of missing values:"
sum(is.na(df))
# "N. of duplicated rows:"
sum(duplicated(df))
# "Example of some objects:"
head(df, 3)
```

From the above table it can be seen that all the column have the right format now. Also can be observed that there are no missing values but some duplicate rows are present. Duplicates in this case probably refers to students that have the same profile conditions and that have replayed in the same way to the survey for building the dataset. It's choosen to keep these rows to mantain the original dataset semantics.

Talking about the summary of the dataset different aspects can be observed:

- The majority of the students are female (67.5\%)
- A lot of person have at least one brother or sisters (72.8\%)
- Less students in the dataset are from Medium to Large Cities (10.2\%)
- The number of students with average family economic status is the highest (66.8\%), instead the number of students with rich family it quite low (0.4\%)
- The most common major is Liberal (58.5\%)
- Only 0.3\% of the person are undergraduate with grade five, instead the other grades are quite equally distributed
- The mean value of the variable Internet addiction is near 10 (scale [5, 20]) while the means for sleeping disturbance, impulsivity, compulsion, dependence are around 7 (scale [4, 16]). These symptoms scores are a bit higher with respect to the other variables values
- No peoples with self-injury behaviors has symptoms of value 15 and they have the lowest average score (4.82), suggesting it might be the least prevalent issue
- In general students with very high symptoms are fortunately the minority among the observed data sample, so the numeric distribution is right skewed.


For convenience, in this analysis it's decided to transform the variable *suicide* in a binary feature, considering the current problem as a classification task.<br/>
In this sense, the values of *suicide* lower or equal to 8 take the value FALSE, instead the other take the value TRUE. The choice of this threshold is considered reasonable because the purpose of this analysis is to consider higher severity of suicide but at the same time don't underestimate the risk even if the value of severity is not too high.


```{r}
print(summary(df$suicide))

threshold_suicide <- 8
numeric_suicide <- df$suicide
scaled_suicide <- numeric_suicide - min(numeric_suicide)
factor_suicide <- as.factor(df$suicide > threshold_suicide)
df$suicide <- factor_suicide

print(summary(df$suicide))
```


# Exploratory Data Analysis
After a first overview of the data it's needed to inspect some possible relations and hidden pattern between the variables. Some guessed interaction terms are also checked.

## Frequencies with respect to response variable

### General information columns
```{r fig.width=8, fig.height=4, message=FALSE, warning=FALSE, echo=FALSE}
for (col in general_cols) {
  plot_freq_by_category(df, col, "suicide")
}
```

Looking at the bar plots referring to the general information about the students, it seems that there are no strange relationship between the categories and the values of the frequencies for the variable *suicide*: in particular the higher number of suicide for a categorical level is usually related with the higher number of person in that level.
Just for the grade *Freshmen* the number of suicide is higher than the other levels, and in particular there are less suicide for lower grading. Grade variable it's possibly related to some age information that in the dataframe is not present and it could suggest that yunger person are less prone to suicide. 


```{r fig.width=8, fig.height=4, message=FALSE, warning=FALSE, echo=FALSE}
for (col in general_cols) {
  plot_proportion_by_category(df, col, "suicide")
}
```

The plots above regarding the proportions of students for each category divided by suidice response values instead highlight interesting patterns: indeed it's more clear to understand the relationships by these plots that consider the ratios and not the frequencies because the y scale is large.

There are few visible things by this visualization:

- The relative difference in suicide frequency considering the *gender* seems not so significant based on these data, instead the gap is more noticeable compared with *whether only child*
- It seems that the percentage of suicide in *Medium to Large cities* is higher.
- Being in a rich family lead to higher proportion of suicide based on this dataset
- Attempting the Liberal or the Art major have similar suicide ratio and this is higher with respect to science courses
- As observed before the percentage of positive values in the response variable is higher for freshman category 


### Symptoms information columns
```{r fig.width=8, fig.height=4, message=FALSE, warning=FALSE, echo=FALSE}
for (col in symptoms_cols) {
  plot_freq_by_category(df, col, "suicide")
}
```

The histograms of the symptoms features show a slightly different behavior compared with the other categorical predictors. As observed in the beginning the number of high serverity symptoms in general is low but the frequency of suicide get bigger when these symptoms increase (compulsion, internet addition, sleeping disturbance)


```{r fig.width=8, fig.height=4, message=FALSE, warning=FALSE, echo=FALSE}
for (col in symptoms_cols) {
  plot_proportion_by_category(df, col, "suicide")
}
```


Checking the proportion histograms divided by the response variable what stated in the previously becomes more evident: now it can be seen noticeable increasing trend in the suicide ratios when the symtoms severity increases and in particular the percentage are higher for the variables psyciatric symptoms, hostile aggression, self-injury behaviours and eating problems, even if the frequency of the higher symtoms for them is quite low (except for compulsion)


## Numeric variables
```{r fig.width=8, fig.height=4, message=FALSE, warning=FALSE, echo=FALSE}
corr_matrix <- round(cor(df[, symptoms_cols]), 2)
ggcorrplot(corr_matrix,
  hc.order = TRUE,
  type = "lower",
  lab = TRUE,
  lab_size = 3,
  colors = c("#6D9EC1", "white", "#E46726"),
  title = "Pearson correlation",
  ggtheme = theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))
)
```

The heatmap of the correlation matrix shows that all the variables are quite highly positive correlated with each other. By the plot can be seen that the most correlated variables are *impulsivity* and *compulsion* (0.72), *dependence* and *impulsivity* (0.71), *dependence* and *compulsion* (0.68). Also *self-injury behaviours* and *eating problems* seems to be correlated in a significant way (0.66).

Correlation coefficient significance test is performed to check if for these features correlation is statistically significant. The formula for the t-statistic is: 

$$t_c=\frac{r}{\sqrt{\frac{1-r^2}{n-2}}}= \frac{r \sqrt{n-2}}{\sqrt{1-r^2}}$$
where \(r\) is the correlation coefficient and \(n\) is the number of observations. The null hypothesis is that the correlation coefficient is equal to zero, and the alternative hypothesis is that the correlation coefficient is not equal to zero. The degrees of freedom for the t-distribution are \(n-2\). The p-value is calculated as the probability of observing a t-statistic as extreme as the one calculated from the data, assuming the null hypothesis is true.
```{r}
cor.test(df$impulsivity, df$compulsion)
cor.test(df$dependence, df$impulsivity)
cor.test(df$dependence, df$compulsion)
cor.test(df$self_injury_behaviors, df$eating_problems)
cor.test(df$self_injury_behaviors, df$internet_addiction)
```

The p-values for the correlation tests are all very close to zero, indicating that the correlations are statistically significant.
However, this information it's not fully considered because numeric variables have all positive variables and they are almost all in the same scale.


## Interaction terms
Just for better understanding the possible interaction between the variables, some scatter plots are drawn to check if there are some evident patterns that can be useful for the model development. For this purpose, the response variable is considered again as a numeric variable and a line is fitted for highlight the linear relation between variables divided by categories.

In particular the following interactions are considered:

- **Psychiatric symptoms and gender**: It's important  to inspect these variables because psychiatric conditions often manifest differently across genders, potentially leading to gender-specific patterns in symptoms and treatment outcomes.

- **Eating problems and family economic status**: Economic factors can affect access to resources and support for managing eating problems, influencing the severity and consequences of these issues in different socioeconomic groups.

- **Sleeping disturbance and major**: Different majors may have varying stress levels and workload demands, which can interact with sleep disturbances to affect academic performance and mental health uniquely in each field of study.

- **Hostile aggression and birth place**: Cultural norms around aggression and conflict resolution can vary significantly by location, impacting how hostile aggression influences social relationships and legal issues in different birthplaces.
```{r}
df$suicide <- numeric_suicide
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
ggplot(df, aes(x = psychiatric_symptoms, y = suicide, color = gender)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(group = gender)) +
  labs(
    title = "Suicide vs psychiatric symptoms by gender interaction",
    x = "Psychiatric symptoms",
    y = "Suicide"
  )
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
ggplot(df, aes(x = eating_problems, y = suicide, color = family_economic_status)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(group = family_economic_status)) +
  labs(
    title = "Suicide vs eating problems by family economic status interaction",
    x = "Eating problems",
    y = "Suicide"
  )
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
ggplot(df, aes(x = sleeping_disturbance, y = suicide, color = major)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(group = major)) +
  labs(
    title = "Suicide vs sleeping disturbance by major interaction",
    x = "Sleeping disturbance",
    y = "Suicide"
  )
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
ggplot(df, aes(x = hostile_aggression, y = suicide, color = birth_place)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(group = birth_place)) +
  labs(
    title = "Suicide vs hostile_aggression by birth_place interaction",
    x = "Birth place",
    y = "Suicide"
  )
```

```{r}
df$suicide <- factor_suicide
```

For these data, it seems that just the interaction between *eating problems and family economic status* and *sleeping disturbance and major* is relevant.

# Models developement
In a statistical analysis, splitting data into training and testing sets is crucial for model development. This process allows us to build the model using the training set and then evaluate its performance on the unseen testing set. The hyperparameter tuning where required is instead performed by cross validation on the training partition. This it's needed to check if the model generalizes well to new data and provides an unbiased estimate of its predictive accuracy.

The reference level for the categorical predictors in a full model is a female student with brothers/sisters, from the countryside with extremely poor family, liberal major and postgraduate.

```{r}
idx <- sample(nrow(df), 0.80 * nrow(df))
df_train <- df[idx, ]
df_test <- df[-idx, ]

# Training split dimension
dim(df_train)
# Testing split dimension
dim(df_test)

# Reference levels
for (i in general_cols) {
  print(paste0(i, ": ", levels(df[, i])[1], sep = ""))
}
```


## Logistic regression
Logistic regression is used for modeling directly the probability that the response variable belong to a particluar class.
The coefficients of this generalized linear model in this case represent the log-odds increment/decrement of the outcome (suicide) for a one-unit increase in the predictor variable, holding all other variables constant. A positive coefficient indicates an increase in the log-odds of suicide, while a negative coefficient indicates a decrease.

### Full model
```{r}
mod_full <- glm(suicide ~ ., data = df_train, family = "binomial")
summary(mod_full)
```

Examining the model that includes all the variables, several predictors appear to be strongly significant. Notably, the intercept, which corresponds to the reference level and numeric predictors with coefficients of 0, has a log-odds value of `r mod_full$coefficients[1]`. When converted, this log-odds value represents a probability of `r exp(mod_full$coefficients[1])/(1-exp(mod_full$coefficients[1]))` for the response variable $\mathcal{P}(suicide)=\frac{e^{\beta_0}}{1+e^{\beta_0}}$.


In particular the following model interpretation is considered:

- Being male decreases the log-odds of suicide compared to being female. This suggests that males are less likely to report suicidal tendencies than females in this dataset
- Students from towns, small cities, and medium to large cities have higher log-odds of suicide compared to students from rural areas. This indicates that urbanization might be associated with higher suicidal tendencies.
- Being a science major decreases the log-odds of suicide compared to the reference category
- Freshmen grade has the highest value for the coefficient and it's strongly significant.
- All the symptoms predictor are significant of strongly significant except for eating_problems. Higher levels of psychiatric symptoms, impulsivity, compulsion, sleeping disturbances, internet addiction, hostile aggression, and self-injury behaviors are associated with increased log-odds of suicide. Dependence, however, shows a negative relationship, suggesting that higher dependence decreases the log-odds of suicide for these data.

The above interpretation can be also visualized checking the effect plots, paying attention to the fact that now the y axis represent the response scale (not the log-odds)

```{r}
for (i in general_cols) {
  print(plot(effect(i, mod_full), ylab = "Probability of Suicide", rescale.axis = FALSE))
}
```

```{r}
for (i in symptoms_cols) {
  print(plot(effect(i, mod_full), ylab = "Probability of Suicide", rescale.axis = FALSE))
}
```


### Manual selection

Given the available information, an attempt in developing a model manually is proposed.
Specifically, ....
model 1 contains interaction terms and main effects and impulsivity that in the corr matrix seems the most correlated variable

model 2 contains some possible variables that appear more discriminative and influential

```{r}
mod_manual1_interaction <- glm(
  suicide ~ impulsivity + self_injury_behaviors + family_economic_status +
    eating_problems + major + sleeping_disturbance +
    family_economic_status:eating_problems + major:sleeping_disturbance,
  data = df_train,
  family = "binomial"
)
summary(mod_manual1_interaction)

mod_manual2 <- glm(
  suicide ~ gender + grade + birth_place + impulsivity + self_injury_behaviors +
    hostile_aggression + psychiatric_symptoms,
  data = df_train,
  family = "binomial"
)
summary(mod_manual2)

print(BIC(mod_full, mod_manual1_interaction, mod_manual2))
```
Including the interaction terms and the variables considered in the scatter plots seems to leed no benefit to the model because they seems not significance based on these data. Also the BIC score that penalize more the complex model is higher compared with the model with all predictors. The model including the other predictors selected manually are all strongly significant except some levels in the variable grade. The information criteria is higher also in this case with respect to the full model.


### Stepwise regression
Bidirectional Stepwise Regression, is a variable selection method that combines both forward and backward stepwise regression approaches. It iteratively adds and removes predictors based on a predefined information criterion until no further improvements are observed. This method provides a balance between computational efficiency and model performance, making it suitable for situations where automated variable selection is desired while controlling for overfitting.

```{r}
# AIC stepwise regression
mod_zero <- glm(suicide ~ 1, data = df_train, family = "binomial")
mod_step_aic <- stepAIC(mod_zero, direction = "both", scope = list(upper = formula(mod_full)), trace = 0)
summary(mod_step_aic)
```

The logistic regression model identifies several significant predictors of suicide among students. Psychiatric symptoms, sleeping disturbance, self-injury behaviors, compulsion, grade (especially freshmen and sophomores), hostile aggression, certain birth places, internet addiction, and gender show significant associations with the likelihood of suicide. Notably, being a science major and higher dependence are associated with lower likelihoods of suicide. The model seems to have a good explainable power as indicated the the slight reduction of the AIC score with respect to full model, however the predictors are still a bit (13 features vs 16 total).

It's possible to try stepwise regression using BIC information criteria to select a more parsimonius model
```{r}
# BIC stepwise regression
mod_step_bic <- stepAIC(mod_zero, direction = "both", scope = list(upper = formula(mod_full)), trace = 0, k = log(nrow(df_train)))
summary(mod_step_bic)
```

Both models consistently identify psychiatric symptoms, sleeping disturbance, self-injury behaviors, compulsion, impulsivity, dependence, gender, and hostile aggression as significant predictors.
The AIC model includes additional significant predictors not found in the BIC model, such as certain grades (especially Freshman), some birth places, internet addiction, and specific majors. The choice between the models depends on the preference for model complexity versus simplicity and the specific goals of the analysis.

For completeness, effect plots from the stepwise regression model with AIC are provided:

```{r}
for (i in extract_predictors_in_vec(mod_step_aic$formula, general_cols)) {
  print(plot(effect(i, mod_step_aic), ylab = "Probability of Suicide", rescale.axis = FALSE))
}
```

```{r}
for (i in extract_predictors_in_vec(mod_step_aic$formula, symptoms_cols)) {
  print(plot(effect(i, mod_step_aic), ylab = "Probability of Suicide", rescale.axis = FALSE))
}
```



### Lasso shrinkage method
Lasso introduces a penalty term to the standard regression objective function, encouraging sparsity by shrinking the coefficients of less important predictors towards zero. This regularization actually perform a feature selection, making it particularly useful when dealing with high-dimensional datasets with potentially correlated predictors.


```{r}
mod_lasso <- cv.glmnet(suicide ~ ., data = df_train, family = "binomial", alpha = 1)
beta <- coef(mod_lasso, s = mod_lasso$lambda.1se)
beta_1se_idx <- which(beta[, 1] != 0)
print(cbind(beta_1se_idx, beta[beta_1se_idx])[, 2])
```

The fitted logistic regression model suggests that several factors, including being a freshman, the presence of psychiatric symptoms, impulsivity, compulsion, sleeping disturbances, hostile aggression, and self-injury behaviors, contribute to an increased likelihood of experiencing mental health issues among students. In particular the coefficients of psychiatric_symptoms and self_injury_behaviors are higher than the others. Conversely, being born in the countryside has a very small negative effect on the likelihood of experiencing these issues. Also in this case the log-odds value of the intercept is negative.<br/>
As can be observed the majority coefficients keept by the model are related to symptoms features, instead just the level country side of the variable bith_place and the grade Freshman seems relevant for these data.

If compared to the coefficients selected from stepwise regression and BIC information criteria, the magnitude of the common coefficients is smaller, indicating that the lasso model tends to shrink the coefficients towards zero as extected. In addition the two models vary just by the selection of dependence and gender for BIC and the selection of birth place (countryside) and grade (freshman) for Lasso, means that the two models are quite similar but highlight some different predictors.


Looking at the below plots can be seen the the coefficients shrinkage with respect to the log lambda value and the behaviour of the binomial deviance with respect to this hyper parameter. The dotted line represents the log lambda value that minimizes the cross-validated error, while the dashed line represents the log lambda value that is within one standard error of the minimum.<br/>
The plots show that the model is quite stable with respect to the lambda value, and the coefficients are not drastically affected by the regularization.

It's decided to take the model with the lambda value plus one standard error because it's more parsimonious and the difference in the binomial deviance is not so high with respect to the minimum value.
```{r}
plot(mod_lasso$glmnet.fit, xvar = "lambda")
abline(v = log(mod_lasso$lambda.min), lty = "dotted", col = "darkorange")
abline(v = log(mod_lasso$lambda.1se), lty = "dashed", col = "steelblue")
legend("topright",
  legend = c("lambda.min", "lambda.1se"),
  col = c("darkorange", "steelblue"),
  lty = c("dotted", "dashed"),
  cex = 0.8
)

plot(mod_lasso)
abline(v = log(mod_lasso$lambda.min), lty = "dotted", col = "darkorange")
abline(v = log(mod_lasso$lambda.1se), lty = "dashed", col = "steelblue")
legend("topright",
  legend = c("lambda.min", "lambda.1se"),
  col = c("darkorange", "steelblue"),
  lty = c("dotted", "dashed"),
  cex = 0.8
)
```

Notice that the implementation of Lasso of glmnetUtils deliberately avoids the usual treatment of factors with a reference level “absorbed” by the intercept. The model considered it's more interpretable because otherwise the method could cancel the differences in level of factor predictors


## Generative models
After modelling directly the response variable by logistic regression based model, it's possible to consider generative models that estimates the class-specific densities and then uses Bayes' theorem to estimate the posterior probabilities of the classes. These type of models are more stable and robust when there's a substantial separation between the classes as probably in this case.

To enhance interpretability for all the generative model, the predictios selected by stepwise regression with BIC information criteria are used in the model fitting.
 

### Linear Discriminant Analysis (LDA)
The LDA model assumes that the predictors are normally distributed and that the covariance matrices are equal across classes. 
```{r}
mod_lda <- lda(mod_step_bic$formula, data = df_train)
mod_lda
```

### Quadratic Discriminant Analysis (QDA)
The QDA model generalize the LDA assuming different covariance matrices for each class, so the flexibility is increased but the number of parameters to estimate is much larger

```{r}
mod_qda <- qda(mod_step_bic$formula, data = df_train)
mod_qda
```

### Naive Bayes
Naive Bayes instead make the assumption of independence between the predictors to estimates the class-specific densities, this assumption usually introduces bias but reduce the variance component of the error

```{r}
mod_nb <- naiveBayes(mod_step_bic$formula, data = df_train)
mod_nb
```


## Other models

### KNN
Another option is to use K-Nearest Neighbors (KNN) algorithm, a non-parametric method that works by finding the K closest data points in the training set to a given data point in the testing set and making predictions based on the most common class among its neighbors. It is a versatile and intuitive method but can be computationally expensive for large datasets.

Also for KNN model the predictor used are the one selected by stepwise regression with BIC information criteria
```{r}
# as.character(mod_step_bic$formula)[-2] extract the predictors and the tilde symbol from the formula
knn_formula <- formula(paste(as.character(mod_step_bic$formula)[-2], collapse = ""))

x.train <- model.matrix(knn_formula, data = subset(jitter_subset(df_train, symptoms_cols), select = -suicide))[, -1]
x.test <- model.matrix(knn_formula, data = subset(jitter_subset(df_test, symptoms_cols), select = -suicide))[, -1]

k_max <- 3
rates <- double(k_max)
for (i in 1:k_max) {
  tmp <- knn(train = x.train, test = x.test, cl = df_train$suicide, k = i, use.all = FALSE)
  rates[i] <- mean(tmp == df_test$suicide)
}
plot(x = (1:k_max), y = rates, xlab = "k", ylab = "Accuracy", type = "l")

best_k <- which.max(rates)
```


### Generalized Additive Model (GAM)
GAMs extend traditional linear models by allowing for non-linear relationships between predictors and the response variable through the use of smooth functions. This flexibility enables the model to capture complex patterns in the data without assuming a specific parametric form.

S() has bs="tp" by default

Thin plate regression splines
bs="tp". These are low rank isotropic smoothers of any number of covariates. By isotropic is meant that rotation of the covariate co-ordinate system will not change the result of smoothing. By low rank is meant that they have far fewer coefficients than there are data to smooth. They are reduced rank versions of the thin plate splines and use the thin plate spline penalty. They are the default smooth for s terms because there is a defined sense in which they are the optimal smoother of any given basis dimension/rank (Wood, 2003). Thin plate regression splines do not have ‘knots’ (at least not in any conventional sense): a truncated eigen-decomposition is used to achieve the rank reduction. See tprs for further details.


```{r}
df$suicide <- numeric_suicide
df_train <- df[idx, ]
df_test_gam <- df[-idx, ]

# extract only the predictors from the formula and model the numeric one as smooth terms
gam_formula <- formula(paste(
  "I(suicide>threshold_suicide)",
  "~",
  paste(extract_predictors_in_vec(mod_step_bic$formula, general_cols), collapse = " + "),
  " + ",
  paste(
    "s(",
    extract_predictors_in_vec(mod_step_bic$formula, symptoms_cols),
    ")",
    collapse = "+"
  )
))

mod_gam <- gam(gam_formula, family = binomial, data = df_train)
summary(mod_gam)

# Restore previous state
df$suicide <- factor_suicide
df_train <- df[idx, ]
```


```{r}
plot(mod_gam, shade = TRUE, shade.col = "lightblue")
```



### Different model definition
For the binomial families the response can be specified in both ways:

As a factor: ‘success’ is interpreted as the factor not having the first level (and hence usually of having the second level).

As a two-column integer matrix: the first column gives the number of successes and the second the number of failures.
```{r}
df$scaled_suicide <- scaled_suicide
mod_succ_fail <- glm(
  cbind(scaled_suicide, (max(scaled_suicide) - scaled_suicide)) ~
    .,
  family = "binomial",
  data = subset(df, select = -suicide)[idx, ]
)
summary(mod_succ_fail)

# Restore previous state
df$scaled_suicide <- NULL
```

```{r message=FALSE, warning=FALSE}
probs_succ_fail <- predict(mod_succ_fail, newdata = df_test, type = "response")
roc_succ_fail <- roc(df_test$suicide ~ probs_succ_fail, plot = TRUE, print.auc = TRUE)
coords_succ_fail <- coords(roc_succ_fail, x = "best", ret = c("threshold", "specificity", "sensitivity", "accuracy", "tn", "tp", "fn", "fp"))
print(coords_succ_fail)
```


# Model comparison
The performance of the models is evaluated using the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) metric, which measures the ability of the model to distinguish between the two classes. The higher the AUC-ROC value, the better the model's predictive performance. The optimal threshold for the response variable is determined by the point on the ROC curve that maximizes the Youden's J statistic, which balances the true positive and false positive rates.

```{r message=FALSE, warning=FALSE}
probs_stepwise_aic <- predict(mod_step_aic, newdata = df_test, type = "response")
roc_stepwise_aic <- roc(df_test$suicide ~ probs_stepwise_aic, plot = TRUE, print.auc = TRUE)
coords_stepwise_aic <- coords(roc_stepwise_aic, x = "best", ret = c("threshold", "specificity", "sensitivity", "accuracy", "tn", "tp", "fn", "fp"))
print(coords_stepwise_aic)
```

```{r message=FALSE, warning=FALSE}
probs_stepwise_bic <- predict(mod_step_bic, newdata = df_test, type = "response")
roc_stepwise_bic <- roc(df_test$suicide ~ probs_stepwise_bic, plot = TRUE, print.auc = TRUE)
coords_stepwise_bic <- coords(roc_stepwise_bic, x = "best", ret = c("threshold", "specificity", "sensitivity", "accuracy", "tn", "tp", "fn", "fp"))
print(coords_stepwise_bic)
```

```{r message=FALSE, warning=FALSE}
probs_lasso <- predict(mod_lasso, newdata = df_test, s = mod_lasso$lambda.1se, type = "response")
roc_lasso <- roc(df_test$suicide ~ as.vector(probs_lasso), plot = TRUE, print.auc = TRUE)
coords_lasso <- coords(roc_lasso, x = "best", ret = c("threshold", "specificity", "sensitivity", "accuracy", "tn", "tp", "fn", "fp"))
print(coords_lasso)
```

```{r message=FALSE, warning=FALSE}
probs_lda <- predict(mod_lda, newdata = df_test)
roc_lda <- roc(df_test$suicide ~ probs_lda$posterior[, 2], plot = TRUE, print.auc = TRUE)
coords_lda <- coords(roc_lda, x = "best", ret = c("threshold", "specificity", "sensitivity", "accuracy", "tn", "tp", "fn", "fp"))
print(coords_lda)
```

```{r message=FALSE, warning=FALSE}
probs_qda <- predict(mod_qda, newdata = df_test)
roc_qda <- roc(df_test$suicide ~ probs_qda$posterior[, 2], plot = TRUE, print.auc = TRUE)
coords_qda <- coords(roc_qda, x = "best", ret = c("threshold", "specificity", "sensitivity", "accuracy", "tn", "tp", "fn", "fp"))
print(coords_qda)
```

```{r message=FALSE, warning=FALSE}
probs_nb <- predict(mod_nb, newdata = df_test, type = "raw")
roc_nb <- roc(df_test$suicide ~ probs_nb[, 2], plot = TRUE, print.auc = TRUE)
coords_nb <- coords(roc_nb, x = "best", ret = c("threshold", "specificity", "sensitivity", "accuracy", "tn", "tp", "fn", "fp"))
print(coords_nb)
```

```{r message=FALSE, warning=FALSE}
probs_knn <- knn(train = x.train, test = x.test, cl = df_train$suicide, k = best_k)
conf_matrix_knn <- table(df_test$suicide, probs_knn)
# Calculate True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN)
TP_knn <- conf_matrix_knn[2, 2] # True Positives
TN_knn <- conf_matrix_knn[1, 1] # True Negatives
FP_knn <- conf_matrix_knn[1, 2] # False Positives
FN_knn <- conf_matrix_knn[2, 1] # False Negatives

# Specificity (True Negative Rate)
specificity <- TN_knn / (TN_knn + FP_knn)
# Sensitivity (Recall or True Positive Rate)
sensitivity <- TP_knn / (TP_knn + FN_knn)
# Accuracy
accuracy <- (TP_knn + TN_knn) / sum(conf_matrix_knn)
coords_knn <- data.frame(threshold = NA, specificity = specificity, sensitivity = sensitivity, accuracy = accuracy, tn = TN_knn, tp = TP_knn, fn = FN_knn, fp = FP_knn)
print(coords_knn)
```

```{r message=FALSE, warning=FALSE}
probs_gam <- predict(mod_gam, newdata = df_test_gam, type = "response")
roc_gam <- roc(df_test_gam$suicide ~ probs_gam, plot = TRUE, print.auc = TRUE)
coords_gam <- coords(roc_gam, x = "best", ret = c("threshold", "specificity", "sensitivity", "accuracy", "tn", "tp", "fn", "fp"))
print(coords_gam)
