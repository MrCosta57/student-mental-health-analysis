---
title: "Mind Matters: A Data-driven Examination of College Students' Mental Health"
subtitle: "Statistical Inferecence and Learining project"
author: "Giovanni Costa - 880892"
date: "AY 2023/24"
geometry: "left=1cm,right=2cm,top=1cm,bottom=1cm"
output:
  html_document:
    toc: true
    number_sections: true
    toc_depth: 2
    toc_float:
      smooth_scroll: false
    fig_caption: yes
    theme: flatly
    highlight: pygments
    css: "assets/css/styles.css"
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align = "center")
```


# Introduction
This project aims to delve into the pressing issue of mental health among college students, a topic of significant importance in today's fast-paced, high-stress academic environment. The analysis seeks to provide a comprehensive understanding of the mental health landscape, by utilizing one dataset coming from a survey, with a particular focus on psychiatric symptoms and other associated health problems.

The dataset, that can be retrieved <a href="https://psycharchives.org/en/item/5d25cf8a-7910-4579-9e39-688c6bbec43a">here</a>, contains 27014 objects with 16 features. The first six information are categorical and related to the general individual condition, instead, the others are numeric variables associated with the mental health of the students, where the higher score indicates more serious symptoms. Additional details regarding the data scales, the data origin, and the retrieving methodology are not provided by the dataset’s website.<br/>
A specific description of these features is provided below:

1. **gender**: 0=Female, 1=Male
2. **whether_only_child**: 0=No, 1=Yes
3. **birth_place**: 0=Countryside, 1=Town, 2=SmallCity, 3=MediumToLargeCities
4. **family_economic_status**: 0=ExtremelyPoor, 1=Poor, 2=Average, 3=Good, 4=Rich
5. **major**: 0=Liberal, 1=Science, 2=Art
6. **grade**: 0=Postgraduate, 1=UndergraduateGradeFive, 2=Junior, 3=Sophomore, 4=Freshman, 5=Senior
7. **psychiatric_symptoms**: range from 4 to 16
8. **suicide**: range from 4 to 16
9. **dependence**: range from 4 to 16
10. **impulsivity**: range from 4 to 16
11. **compulsion**: range from 4 to 16
12. **sleeping_disturbance**: range from 4 to 16
13. **internet_addiction**: range from 5 to 20
14. **hostile_aggression**: range from 4 to 16
15. **self_injury_behaviors:** range from 4 to 16
16. **eating_problems**: range from 4 to 16

The purposes of this analysis are inspecting the possible hidden patterns present in the data and predicting the risk of observing a suicide, modeled for convenience as a binary variable. 

# Prerequirements
```{r results='FALSE', message=FALSE, warning=FALSE}
# Configure the environment
# options(digits = 4, scipen = 999)
rm(list = ls())
# Set seed for reprocucibility
seed <- 123
num_k_fold <- 5
set.seed(seed)

# Load all the packages and install them if they are not present
requirements <- c(
  "summarytools", "MASS", "effects", "pROC", "mgcv",
  "glmnetUtils", "e1071", "class", "car", "brglm2",
  "gridExtra", "grid", "stringr", "ggplot2", "reshape2", "scales", "comprehenr",
  "dplyr", "ggcorrplot"
)
for (library_name in requirements) {
  if (!require(library_name, character.only = TRUE)) {
    install.packages(library_name, repos = "https://cloud.r-project.org")
    library(library_name, character.only = TRUE)
  }
}
# Import user defined functions
source("src/utils.R")
source("src/plotting.R")
# library(styler)
# style_file("analysis_report.Rmd")
```


# First overview

In this section, exploratory data analysis (EDA) is performed to understand the dataset structure and identify any potential issues that need to be addressed. Also, variables are transformed into the appropriate format.

```{r}
# Read the dataset
df <- read.csv("data/mental_health_data.csv")
# Remove the column representing the row index
df <- df[, -1]

# Rename existing columns
colnames(df)[colnames(df) == "whether.only.child"] <- "whether_only_child"
colnames(df)[colnames(df) == "birth.place"] <- "birth_place"
colnames(df)[colnames(df) == "family.economic.status"] <- "family_economic_status"
colnames(df)[colnames(df) == "psychiatric.symptoms"] <- "psychiatric_symptoms"
colnames(df)[colnames(df) == "sleeping.disturbance"] <- "sleeping_disturbance"
colnames(df)[colnames(df) == "internet.addiction"] <- "internet_addiction"
colnames(df)[colnames(df) == "hostile.aggression"] <- "hostile_aggression"
colnames(df)[colnames(df) == "self.injury.behaviors"] <- "self_injury_behaviors"
colnames(df)[colnames(df) == "eating.problems"] <- "eating_problems"

# Transforming variables into factors
df$gender <- factor(df$gender,
  levels = c(0, 1),
  labels = c("Female", "Male")
)
df$whether_only_child <- factor(df$whether_only_child,
  levels = c(0, 1),
  labels = c("No", "Yes")
)
df$birth_place <- factor(
  df$birth_place,
  levels = 0:3,
  labels = c("Countryside", "Town", "SmallCity", "MediumToLargeCities")
)
df$family_economic_status <- factor(
  df$family_economic_status,
  levels = 0:4,
  labels = c("ExtremelyPoor", "Poor", "Average", "Good", "Rich")
)
df$major <- factor(df$major,
  levels = 0:2,
  labels = c("Liberal", "Science", "Art")
)
df$grade <- factor(
  df$grade,
  levels = 0:5,
  labels = c(
    "Postgraduate",
    "UndergraduateGradeFive",
    "Junior",
    "Sophomore",
    "Freshman",
    "Senior"
  )
)
general_cols <- c(
  "gender", "whether_only_child", "birth_place",
  "family_economic_status", "major", "grade"
)
symptoms_cols <- c(
  "psychiatric_symptoms", "dependence", "impulsivity", "compulsion", "sleeping_disturbance",
  "internet_addiction", "hostile_aggression", "self_injury_behaviors", "eating_problems"
)
```

```{r}
print_summary_custom(df)
```

```{r}
# "Dataset dimensions:"
dim(df)
# "N. of missing values:"
sum(is.na(df))
# "N. of duplicated rows:"
sum(duplicated(df))
# "Example of some objects:"
head(df, 3)
```

From the above table it can be seen that all the column have the expected format now. Also can be observed that there are no missing values but some duplicate rows are present. Duplicates in this case probably refers to students that have the same profile conditions and that have replayed in the same way to the survey for building the dataset. It's choosen to keep these rows to mantain the original dataset semantics.

Talking about the summary of the dataset different aspects can be observed:

- The majority of the students are female (67.5\%)
- A lot of people have at least one brother or sister (72.8\%)
- Fewer students in the dataset are from medium to large cities (10.2\%)
- The number of students with average family economic status is the highest (66.8\%), instead the number of students with rich family it quite low (0.4\%)
- The most common major is liberal (58.5\%)
- Only 0.3\% of the people are undergraduates with grade five, instead the other grades are quite equally distributed
- The mean value of the variable Internet addiction is near 10 (scale [5, 20]) while the mean for sleeping disturbance, impulsivity, compulsion, and dependence are around 7 (scale [4, 16]). These symptom scores are a bit higher with respect to the other variables values
- No peoples with self-injury behaviors has symptoms of value 15 and they have the lowest average score (4.82), suggesting it might be the least prevalent issue
- In general students with very high symptoms are fortunately the minority among the observed data sample, so the numeric distributions are right skewed.


For convenience, in this analysis, it's decided to transform the variable *suicide* into a binary feature, considering the current problem as a classification task.<br/>
In this sense, the values of *suicide* lower or equal to 9 take the value *FALSE*, instead the other takes the value *TRUE*. The choice of this threshold is considered reasonable because the purpose of this analysis is to consider the higher severity of suicide but at the same time don't underestimate the risk even if the value is not too big.


```{r}
print(summary(df$suicide))
threshold_suicide <- 9
numeric_suicide <- df$suicide
factor_suicide <- as.factor(df$suicide > threshold_suicide)
df$suicide <- factor_suicide

# Number of suicide
print(table(df$suicide))
# Percentage of suicide
print(table(df$suicide) / nrow(df) * 100)
```

Imposing this threshold for the response variable, the classes apperas highly unbalanced: indeed the proportion of negative suicide in the dataset is `r table(df$suicide)[0] / nrow(df) * 100`\% and the proportion of positive suicide is `r table(df$suicide)[0] / nrow(df) * 100`\%.
Furthermore in the model developement section, some consideration on that will be stated.


# Exploratory Data Analysis
After a first overview of the data, it's needed to inspect some possible relations and hidden patterns between the variables. Some hypothesized interaction terms are also checked.

## Frequencies concerning response variable

### General information columns
```{r fig.height=4, fig.width=8, message=FALSE, warning=FALSE, echo=FALSE}
for (col in general_cols) {
  plot_freq_by_category(df, col, "suicide")
}
```

Looking at the bar plots referring to the general information about the students, it seems that there are no strange relationships between the categories and the values of the frequencies for the variable *suicide*: in particular the higher number of suicides for a categorical level is usually related with the higher number of person in that level.
Just for the grade *Freshmen* the number of suicides is higher than the other levels, and in particular, there are fewer suicides for lower grading. Also, the *grade* variable is possibly related to some age information that in the dataframe is not present and it could suggest that younger people are less prone to suicide ideal. 


```{r fig.height=4, fig.width=8, message=FALSE, warning=FALSE, echo=FALSE}
for (col in general_cols) {
  plot_proportion_by_category(df, col, "suicide")
}
```

The plots above regarding the proportions of students for each category divided by the response values instead highlight interesting patterns: indeed it's more clear to understand the relationships by plots that consider ratios and not the frequencies, also because the y scale is different.

There are a few visible things in these visualizations:

- The relative difference in suicide frequency considering the gender seems not so significant based on these data, instead, the gap is more noticeable compared with *whether only child* variable
- It seems that the percentage of suicide in medium to large cities* is higher.
- Being in a rich family leads to a higher proportion of suicide based on this dataset, even more that being in extremely poor family economic condition
- Attempting the art major has a similar suicide ratio and this is higher with respect to liberal and science courses
- As observed before the percentage of positive values in the response variable is higher for the freshman category 


### Symptoms information columns
```{r fig.height=4, fig.width=8, message=FALSE, warning=FALSE, echo=FALSE}
for (col in symptoms_cols) {
  plot_freq_by_category(df, col, "suicide")
}
```

The histograms of the symptoms features show a slightly different behavior compared with the previous categorical predictors' plots. As observed in the beginning, the number of high-severity symptoms in general is low but the frequency of suicide gets bigger when these symptoms increase.

```{r fig.height=4, fig.width=8, message=FALSE, warning=FALSE, echo=FALSE}
for (col in symptoms_cols) {
  plot_proportion_by_category(df, col, "suicide")
}
```


Checking the proportion histograms divided by the response variable what was stated previously becomes more evident: now it can be seen a noticeable increasing trend in the suicide ratios when the symptoms severity increases and in particular the percentage is higher for the variables psychiatric symptoms, hostile aggression, self-injury behaviors and eating problems, even if the frequency of the higher symptoms for them is quite low (except for impulsivity and compulsion variables)


## Numeric variables
```{r fig.height=4, fig.width=8, message=FALSE, warning=FALSE, echo=FALSE}
corr_matrix <- round(cor(df[, symptoms_cols]), 2)
ggcorrplot(corr_matrix,
  hc.order = TRUE,
  type = "lower",
  lab = TRUE,
  lab_size = 3,
  colors = c("#6D9EC1", "white", "#E46726"),
  title = "Pearson correlation",
  ggtheme = theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))
)
```

The heatmap of the correlation matrix shows that all the variables are quite highly positively correlated with each other. By the plot can be seen that the most correlated variables are *impulsivity* and *compulsion* (0.72), *dependence* and *impulsivity* (0.71), *dependence* and *compulsion* (0.68). Also *self-injury behaviours* and *eating problems* seem to be correlated in a significant way (0.66).

The correlation coefficient significance test is performed to check if for these features correlation is statistically significant. The formula for the t-statistic is: 

$$t_c=\frac{r}{\sqrt{\frac{1-r^2}{n-2}}}= \frac{r \sqrt{n-2}}{\sqrt{1-r^2}}$$
where \(r\) is the correlation coefficient and \(n\) is the number of observations. The null hypothesis is that the correlation coefficient is equal to zero, and the alternative hypothesis is that the correlation coefficient is not equal to zero. The degrees of freedom for the t-distribution are \(n-2\). The p-value is calculated as the probability of observing a t-statistic as extreme as the one calculated from the data, assuming the null hypothesis is true.

```{r}
cor.test(df$impulsivity, df$compulsion)$p.value
cor.test(df$dependence, df$impulsivity)
cor.test(df$dependence, df$compulsion)
cor.test(df$self_injury_behaviors, df$eating_problems)
cor.test(df$self_injury_behaviors, df$internet_addiction)
```

The p-values for the correlation tests are all very close to zero, indicating that the correlations are statistically significant.
However, this information it's not fully considered because numeric variables have all positive variables and they are almost all in the same scale.


## Interaction terms
Just for better understanding the possible interaction between the variables, some scatter plots are drawn to check if there are some evident patterns that can be useful for the model development. For this purpose, the response variable is considered again as a numeric variable and a line is fitted for highlight the linear relation between variables divided by categories.

In particular the following interactions are considered:

- **Psychiatric symptoms and gender**: It's important to inspect these variables because psychiatric conditions often manifest differently across genders, potentially leading to gender-specific patterns in symptoms and treatment outcomes.

- **Eating problems and family economic status**: Economic factors can affect access to resources and support for managing eating problems, influencing the severity and consequences of these issues in different socioeconomic groups.

- **Sleeping disturbance and major**: Different majors may have varying stress levels and workload demands, which can interact with sleep disturbances to affect academic performance and mental health uniquely in each field of study.

- **Hostile aggression and birth place**: Cultural norms around aggression and conflict resolution can vary significantly by location, impacting how hostile aggression influences social relationships and legal issues in different birthplaces.

```{r}
df$suicide <- numeric_suicide
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
ggplot(df, aes(x = psychiatric_symptoms, y = suicide, color = gender)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(group = gender)) +
  labs(
    title = "Suicide vs psychiatric symptoms by gender interaction",
    x = "Psychiatric symptoms",
    y = "Suicide"
  )
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
ggplot(df, aes(x = eating_problems, y = suicide, color = family_economic_status)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(group = family_economic_status)) +
  labs(
    title = "Suicide vs eating problems by family economic status interaction",
    x = "Eating problems",
    y = "Suicide"
  )
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
ggplot(df, aes(x = sleeping_disturbance, y = suicide, color = major)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(group = major)) +
  labs(
    title = "Suicide vs sleeping disturbance by major interaction",
    x = "Sleeping disturbance",
    y = "Suicide"
  )
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
ggplot(df, aes(x = hostile_aggression, y = suicide, color = birth_place)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(group = birth_place)) +
  labs(
    title = "Suicide vs hostile_aggression by birth_place interaction",
    x = "Birth place",
    y = "Suicide"
  )
```

```{r}
df$suicide <- factor_suicide
```

For these data, it seems that just the interaction between *eating problems and family economic status* and *sleeping disturbance and major* is relevant.

# Models developement
In a statistical analysis, splitting data into training and testing sets is crucial for model development. This process allows us to build the model using the training set and then evaluate its performance on the unseen testing set. The hyperparameter tuning where required is instead performed by cross validation on the training partition. This it's needed to check if the model generalizes well to new data and provides an unbiased estimate of its predictive accuracy.

The reference level for the categorical predictors in a full model is a female student with brothers/sisters, from the countryside with an extremely poor family, liberal major and postgraduate.

```{r}
idx <- sample(nrow(df), 0.8 * nrow(df))
df_train <- df[idx, ]
df_test <- df[-idx, ]

print(paste0("Train size: ", nrow(df_train)))
print(paste0("Test size: ", nrow(df_test)))

# Reference levels
for (i in general_cols) {
  print(paste0(i, ": ", levels(df[, i])[1], sep = ""))
}
# Class frequency in the training set
print(table(df_train$suicide))
# Class proportion in the training set
print(table(df_train$suicide) / nrow(df_train) * 100)

# Class frequency in the test set
print(table(df_test$suicide))
# Class proportion in the test set
print(table(df_test$suicide) / nrow(df_test) * 100)
```


## Logistic regression
Logistic regression is used for modeling directly the probability that the response variable belongs to a particular class.
The coefficients of this generalized linear model in this case represent the log-odds change of the outcome (suicide) for a one-unit increase in the predictor variable, holding all other variables constant. A positive coefficient indicates an increase in the log-odds of suicide, while a negative coefficient indicates a decrease.

### Full model
```{r}
mod_full_std <- glm(suicide ~ ., data = df_train, family = "binomial")
summary(mod_full_std)
```

Examining the model that includes all the variables, several predictors appear to be strongly significant. Notably, the intercept, which corresponds to the reference level and numeric predictors with coefficients of 0, has a log-odds value of `r mod_full_std$coefficients[1]`. When converted, this log-odds value represents a probability of `r exp(mod_full_std$coefficients[1])/(1-exp(mod_full_std$coefficients[1]))` for the response variable $\mathcal{P}(suicide)=\frac{e^{\beta_0}}{1+e^{\beta_0}}$.


In particular, the following model interpretation is considered:

- Being male decreases the log-odds of suicide compared to being female. This suggests that males are less likely to report suicidal tendencies than females in this dataset
- Students from towns, small cities, and medium to large cities have higher log-odds of suicide compared to students from rural areas. This indicates that urbanization might be associated with higher suicidal tendencies.
- Being in a family with poor, average or good econimic status leads to a large decrease in the log-odds, instead being in a rich family seems to be not significant in this model 
- Freshmen grade has the highest value for the coefficient and it's strongly significant.
- All the symptoms predictor are significant of strongly significant except for eating_problems. Higher levels of psychiatric symptoms, impulsivity, compulsion, sleeping disturbances, internet addiction, hostile aggression, and self-injury behaviors are associated with increased log-odds of suicide. Dependence, however, shows a negative relationship, suggesting that higher dependence decreases the log-odds of suicide for these data.
- The levels in whether only child and in major have high p-values for the significance test and they seem not influential

```{r}
vif(mod_full_std)
cook_dist <- cooks.distance(mod_full_std)
# Plot using ggplot2
ggplot(
  data.frame(Index = 1:length(cook_dist), Cook = cook_dist),
  aes(x = Index, y = Cook)
) +
  geom_point() +
  geom_line() +
  labs(x = "Observation Index", y = "Cook's Distance") +
  ggtitle("Cook's Distance for Each Observation") +
  theme_minimal()
cook_dist <- NULL
```

The model don't show significantly signs of multicollinearity or infuential points


Models fitted using standard Maximum likelihood method in this case where proportion of classes in response varible is very unbalanced could be not be suitable.
Specifically, to overcome this problem, bias-corrected maximum likelihood^[Kosmidis, I., Kenne Pagui, E.C. & Sartori, N. Mean and median bias reduction in generalized linear models. Stat Comput 30, 43–59 (2020)] technique is utilized for estimating the coefficients of this logistic regression model in a robust manner.

```{r}
mod_zero_br <- glm(suicide ~ 1, data = df_train, family = "binomial", method = "brglmFit")
mod_full_br <- glm(suicide ~ ., data = df_train, family = "binomial", method = "brglmFit")
```
```{r}
tab <- cbind(coef(mod_full_std), coef(mod_full_br))
colnames(tab) <- c("Vanilla Model", "Full Model")
tab
tab <- NULL
```
Models presents slightly differences in the coefficient estimations in this case, but due to the better theorictical properities of the bias reduction methods for logistic regression, this one is utilized for the analysis.


The above interpretation can be also visualized by checking the effect plots, paying attention to the fact that now the y-axis is represented in the response scale (not the log-odds one)

```{r fig.height=25, fig.width=18}
plts <- list()
for (i in general_cols) {
  p <- plot(effect(i, mod_full_br), ylab = "Probability of Suicide", rescale.axis = FALSE)
  plts[[i]] <- p
}
grid.arrange(grobs = plts, top = textGrob("General features effect plots", gp = gpar(fontsize = 20, font = 3)))
plts <- NULL
```

```{r fig.height=25, fig.width=18}
plts <- list()
for (i in symptoms_cols) {
  p <- plot(effect(i, mod_full_br), ylab = "Probability of Suicide", rescale.axis = FALSE)
  plts[[i]] <- p
}
grid.arrange(grobs = plts, top = textGrob("Symptoms features effect plots", gp = gpar(fontsize = 20, font = 3)))
plts <- NULL
```


### Manual selection

Given the available information, an attempt in developing a model manually is proposed.
Specifically, the first model contains some significant variables present in the full model, the previously checked interaction terms that seems influential and their main effects, impulsivity feature that in the corr matrix seems the most correlated variable. The second model instead contains some possible variables that appear more discriminative

```{r}
mod_manual1_interaction <- glm(
  suicide ~ impulsivity + self_injury_behaviors + family_economic_status +
    eating_problems + major + sleeping_disturbance +
    family_economic_status:eating_problems + major:sleeping_disturbance,
  data = df_train,
  family = "binomial",
  method = "brglmFit"
)
summary(mod_manual1_interaction)

mod_manual2 <- glm(
  suicide ~ gender + grade + birth_place + impulsivity + self_injury_behaviors +
    hostile_aggression + psychiatric_symptoms,
  data = df_train,
  family = "binomial",
  method = "brglmFit"
)
summary(mod_manual2)

print(BIC(mod_full_br, mod_manual1_interaction, mod_manual2))
```

Including the interaction terms and the variables considered in the scatter plots seems to lead no benefit to the model because they appear not significant based on these data. Also, the BIC score that penalizes more the complex model, is higher compared with the model with all predictors. The second manual has all significant predictors, except for some levels in the variable grade. The information criteria is higher also in this case with respect to the full model.

For both the manual model, the significant coefficients have the same sign but different values compared with the full model


### Stepwise regression
Bidirectional Stepwise Regression, is a variable selection method that combines both forward and backward stepwise regression approaches. It iteratively adds and removes predictors based on a predefined information criterion until no further improvements are observed. This method provides a balance between computational efficiency and model performance, making it suitable for situations where automated variable selection is desired while controlling for overfitting.

```{r}
# AIC stepwise regression
mod_step_aic <- stepAIC(mod_zero_br, direction = "both", scope = list(upper = formula(mod_full_br)), trace = 0)
summary(mod_step_aic)
```

The logistic regression model identifies several significant predictors of suicide among students. Psychiatric symptoms, sleeping disturbance, self-injury behaviors, compulsion, grade (sophomores, freshmen and senior), hostile aggression, birth places, internet addiction and impulsivity show significant associations with the likelihood of suicide. Notably, being a science major and higher dependence are associated with lower likelihoods of suicide. The model seems to have a good explainable power as indicated the the slight reduction of the AIC score with respect to full model, however the predictors are still a bit (12 features selected vs 15 total features).

It's possible to try stepwise regression using BIC information criteria to select a more parsimonious model

```{r}
# BIC stepwise regression
mod_step_bic <- stepAIC(mod_zero_br, direction = "both", scope = list(upper = formula(mod_full_br)), trace = 0, k = log(nrow(df_train)))
summary(mod_step_bic)
```

Both models consistently identify psychiatric symptoms, sleeping disturbance, self-injury behaviors, compulsion and hostile aggression as significant predictors. The sign of the estimated coefficient is the same for them.
The AIC model as can be deducted includes additional significant predictors not present in the BIC model. The choice between the models depends on the preference for model complexity versus simplicity and the specific goals of the analysis.

For completeness, effect plots from the stepwise regression model with AIC are provided:
```{r fig.height=20, fig.width=18}
plts <- list()
for (i in extract_predictors_in_vec(mod_step_aic$formula, general_cols)) {
  p <- plot(effect(i, mod_step_aic), ylab = "Probability of Suicide", rescale.axis = FALSE)
  plts[[i]] <- p
}
grid.arrange(grobs = plts, top = textGrob("General features effect plots", gp = gpar(fontsize = 20, font = 3)))
plts <- NULL
```

```{r fig.height=25, fig.width=18}
plts <- list()
for (i in extract_predictors_in_vec(mod_step_aic$formula, symptoms_cols)) {
  p <- plot(effect(i, mod_step_aic), ylab = "Probability of Suicide", rescale.axis = FALSE)
  plts[[i]] <- p
}
grid.arrange(grobs = plts, top = textGrob("Symptoms features effect plots", gp = gpar(fontsize = 20, font = 3)))
plts <- NULL
```



### Lasso shrinkage method
Lasso introduces a penalty term to the standard regression objective function, encouraging sparsity by shrinking the coefficients of less important predictors toward zero. This regularization performs a feature selection, making it particularly useful when dealing with high-dimensional datasets with potentially correlated predictors.

Notice that the implementation of Lasso in glmnetUtils library deliberately avoids the usual treatment of factors with a reference level “absorbed” by the intercept. The model considered it's more interpretable because otherwise the method could cancel the differences in the factor levels of the predictors.


```{r}
mod_lasso <- cv.glmnet(suicide ~ ., data = df_train, family = "binomial", method = "brglmFit", alpha = 1, nfolds = num_k_fold)
beta_1se <- coef(mod_lasso, s = mod_lasso$lambda.1se)
beta_1se_idx <- which(beta_1se[, 1] != 0)
print(cbind(beta_1se_idx, beta_1se[beta_1se_idx])[, 2])
print(sum(beta_1se[, 1] != 0))
```

The fitted logistic regression model suggests several factors, all related to symptom variables, including being a freshman, the presence of psychiatric symptoms, and they contribute all positively to a higher risk of suicide.

If compared to the coefficients selected from stepwise regression, the magnitude of the common coefficients is usually smaller, indicating that the lasso model tends to shrink the coefficients towards zero as expected. In addition, BIC and Lasso models select the same predictors.


Looking at the below plots can be seen the coefficients shrinkage with respect to the log lambda value in the first, and the behaviour of the binomial deviance with respect to this hyper parameter in the latter. The dotted line represents the log lambda value that minimizes the cross-validated error, while the dashed line represents the log lambda value that is within one standard error of the minimum.<br/>
It's decided to take the model with the lambda value plus one standard error because it's more parsimonious and the difference in the binomial deviance is not so high with respect to the minimum value.

```{r echo=FALSE}
plot(mod_lasso$glmnet.fit, xvar = "lambda")
abline(v = log(mod_lasso$lambda.min), lty = "dotted", col = "darkorange")
abline(v = log(mod_lasso$lambda.1se), lty = "dashed", col = "steelblue")
legend("topright",
  legend = c("lambda.min", "lambda.1se"),
  col = c("darkorange", "steelblue"),
  lty = c("dotted", "dashed"),
  cex = 0.8
)

plot(mod_lasso)
abline(v = log(mod_lasso$lambda.min), lty = "dotted", col = "darkorange")
abline(v = log(mod_lasso$lambda.1se), lty = "dashed", col = "steelblue")
legend("topright",
  legend = c("lambda.min", "lambda.1se"),
  col = c("darkorange", "steelblue"),
  lty = c("dotted", "dashed"),
  cex = 0.8
)
```


## Generative models
After modeling directly the response variable by logistic regression based model, it's possible to consider generative models that estimate the class-specific densities and then use Bayes' theorem to derive the posterior probabilities of the classes. These types of models are more stable and robust when there's a substantial separation between the classes as probably in this case.

To enhance interpretability for all the generative models, the preditors selected by stepwise regression with BIC information criteria are used in the model fitting.
 

### Linear Discriminant Analysis (LDA)
The LDA model assumes that the predictors are normally distributed and that the covariance matrices are equal across classes. 
```{r}
lda_formula <- mod_step_bic$formula
mod_lda <- lda(lda_formula, data = df_train)
mod_lda
```
The printed output of LDA includes the a-priori probabilities of suicide and the group means.<br/>
The proportion of training observation that belongs to positive suicide variables is fortunately quite low. As previously observed different times, when response variable is true and predictor are the symptom variables, their mean is higher compared with the case in which response variable is false.


### Quadratic Discriminant Analysis (QDA)
The QDA model generalizes the LDA assuming different covariance matrices for each class, so the flexibility is increased but the number of parameters to estimate is much larger

```{r}
qda_formula <- mod_step_bic$formula
mod_qda <- qda(qda_formula, data = df_train)
mod_qda
```

### Naive Bayes
Naive Bayes instead make the assumption of independence between the predictors to estimate the class-specific densities, this assumption usually introduces bias but reduces the variance component of the error. The standard naive Bayes classifier for estimating univariate class densities of quantitative predictors assumes Gaussian distribution (likelihood component)

```{r}
nb_formula <- mod_step_bic$formula
mod_nb <- naiveBayes(nb_formula, data = df_train)
mod_nb
```
The output of naiveBayes contains the estimated a-priori probabilities and a table containing the mean and standard deviation of the numeric variables, conditioned by the response.


## Other models

### KNN
Another option is to use K-Nearest Neighbors (KNN) algorithm, a non-parametric method that works by finding the K closest data points in the training set to a given data point in the testing set and making predictions based on the most common class among its neighbors. It is a versatile and intuitive method but can be computationally expensive for large datasets.

Also for KNN model the predictor used are the one selected by stepwise regression with BIC information criteria
```{r fig.height=4, fig.width=8}
# as.character(mod_step_bic$formula)[-2] extract the predictors and the tilde symbol from the formula
knn_formula <- formula(paste(as.character(mod_step_bic$formula)[-2], collapse = ""))

k_max <- 3
knn_rates <- list()
for (i in 1:k_max) {
  knn_rates[[i]] <- k_fold_cv(
    model_formula = knn_formula, model_func = class::knn, data = df_train,
    is_knn = TRUE,
    k = i, n_fold = num_k_fold, symptoms_cols = symptoms_cols
  )
}
best_knn_k <- NULL
best_knn_sensitivity <- -1
for (i in 1:k_max) {
  if (knn_rates[[i]][["mean_sensitivity"]] > best_knn_sensitivity) {
    best_knn_k <- i
    best_knn_sensitivity <- knn_rates[[i]][["mean_sensitivity"]]
  }
}
print(paste("Best k value:", best_knn_k, ",", "Best sensitivity value:", best_knn_sensitivity, sep = " "))
```


### Generalized Additive Model (GAM)
GAMs extend traditional linear models by allowing for non-linear relationships between predictors and the response variable through the use of smooth functions. This flexibility enables the model to capture complex patterns in the data without assuming a specific parametric form.

In particular, the function `gam` of the R package `mgcv` by default include in the non linear model smooth terms using thin plate regression splines^[Thin plate regression splines are described in detail in Wood (2003), *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, 65(1), 95-114.], where the degree of smoothness is estimated as part of the fitting procedure.

```{r}
# extract only the predictors from the formula and model the numeric one as smooth terms
gam_formula_bic <- formula(paste(
  "suicide~",
  paste(extract_predictors_in_vec(mod_step_bic$formula, general_cols), collapse = " + "),
  " + ",
  paste(
    "s(",
    extract_predictors_in_vec(mod_step_bic$formula, symptoms_cols),
    ")",
    collapse = "+"
  )
))

mod_gam_bic <- gam(gam_formula_bic, family = "binomial", data = df_train)
summary(mod_gam_bic)
```

All the five smooth terms are considered significant according to the p-value and the model explain 36.8% of the deviance. In addition, the estimated effective degrees of freedom (edf) propose complex fits for all the predictors, indeed higher edf values suggest more flexibility in capturing non-linear effects.

This GAM identifies significant non-linear relationships between several mental health factors and the likelihood of suicide attempts among students. Key contributors include psychiatric symptoms, sleeping disturbances, self-injury behaviors, compulsions, and hostile aggression. Each of these predictors exhibits a statistically significant and complex relationship with suicide risk, demonstrating the nuanced influence of mental health symptoms on the probability of suicide attempts. The model captures a moderate proportion of variability in suicide risk, emphasizing the importance of considering non-linear effects in understanding the predictors of suicide among students.

```{r fig.height=8, fig.width=15}
par(mfrow = c(1, 2))
plot(mod_gam_bic, shade = TRUE, shade.col = "lightblue")
par(mfrow = c(1, 1))
```


```{r}
mod_gam_manual <- update(mod_gam_bic, . ~ . + grade + birth_place)
summary(mod_gam_manual)
```

```{r fig.height=8, fig.width=15}
par(mfrow = c(1, 2))
plot(mod_gam_manual, shade = TRUE, shade.col = "lightblue")
par(mfrow = c(1, 1))
```

```{r}
AIC(mod_gam_bic, mod_gam_manual)
```


# Model comparison
The performance of the models is evaluated using the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) metric, which measures the ability of the model to distinguish between the two classes. The higher the AUC-ROC value, the better the model's predictive performance. The optimal threshold for the response variable is determined by the point on the ROC curve that maximizes the Youden's J statistic, which balances the true positive and false positive rates.

```{r}
str_row_names <- c(
  "Full std model", "Full br model", "Manual model 2", "Stepwise regression with AIC", "Stepwise regression with BIC",
  "Lasso", "LDA", "QDA", "Naive Bayes", "KNN", "GAM with BIC", "GAM ma"
)
# Summary of the information criteria when available
info_criteria <- cbind(
  AIC(mod_full_br, mod_manual2, mod_step_aic, mod_step_bic, mod_gam_bic, mod_gam_manual)[, 2],
  BIC(mod_full_br, mod_manual2, mod_step_aic, mod_step_bic, mod_gam_bic, mod_gam_manual)[, 2]
)
colnames(info_criteria) <- c("AIC", "BIC")
rownames(info_criteria) <- str_row_names[c(2, 3, 4, 5, 11, 12)]
info_criteria <- as.data.frame(info_criteria)
```

```{r}
# Sort by best sensitivity (true positive rate)
(info_criteria %>% arrange(AIC))[, c("AIC", "BIC")]
```

```{r message=FALSE, warning=FALSE}
metrics_val_full_std <- k_fold_cv(
  model_formula = mod_full_std$formula, model_func = stats::glm, data = df_train,
  n_fold = num_k_fold,
  family = "binomial"
)
metrics_val_full_br <- k_fold_cv(
  model_formula = mod_full_br$formula, model_func = stats::glm, data = df_train,
  n_fold = num_k_fold,
  family = "binomial", method = "brglmFit"
)
metrics_val_manual2 <- k_fold_cv(
  model_formula = mod_manual2$formula, model_func = stats::glm, data = df_train,
  n_fold = num_k_fold,
  family = "binomial", method = "brglmFit"
)
metrics_val_stepwise_aic <- k_fold_cv(
  model_formula = mod_step_aic$formula, model_func = stats::glm, data = df_train,
  n_fold = num_k_fold,
  family = "binomial", method = "brglmFit"
)
metrics_val_stepwise_bic <- k_fold_cv(
  model_formula = mod_step_bic$formula, model_func = stats::glm, data = df_train,
  n_fold = num_k_fold,
  family = "binomial", method = "brglmFit"
)
metrics_val_lasso <- k_fold_cv(
  model_formula = formula("suicide~."), model_func = glmnetUtils::glmnet, data = df_train,
  n_fold = num_k_fold,
  family = "binomial", method = "brglmFit", lambda = mod_lasso$lambda.1se, alpha = 1
)
metrics_val_lda <- k_fold_cv(
  model_formula = lda_formula, model_func = MASS::lda, data = df_train,
  n_fold = num_k_fold, is_generative = TRUE
)
metrics_val_qda <- k_fold_cv(
  model_formula = qda_formula, model_func = MASS::qda, data = df_train,
  n_fold = num_k_fold, is_generative = TRUE
)
metrics_val_nb <- k_fold_cv(
  model_formula = nb_formula, model_func = e1071::naiveBayes, data = df_train,
  n_fold = num_k_fold, is_nb = TRUE
)
metrics_val_knn <- knn_rates[[best_knn_k]]
metrics_val_gam_bic <- k_fold_cv(
  model_formula = mod_gam_bic$formula, model_func = mgcv::gam, data = df_train,
  n_fold = num_k_fold, family = "binomial"
)
metrics_val_gam_manual <- k_fold_cv(
  model_formula = mod_gam_manual$formula, model_func = mgcv::gam, data = df_train,
  n_fold = num_k_fold, family = "binomial"
)
```

```{r message=FALSE, warning=FALSE}
metrics_val_total <- as.data.frame(rbind(
  unlist(metrics_val_full_std),
  unlist(metrics_val_full_br),
  unlist(metrics_val_manual2),
  unlist(metrics_val_stepwise_aic),
  unlist(metrics_val_stepwise_bic),
  unlist(metrics_val_lasso),
  unlist(metrics_val_lda),
  unlist(metrics_val_qda),
  unlist(metrics_val_nb),
  unlist(metrics_val_knn),
  unlist(metrics_val_gam_bic),
  unlist(metrics_val_gam_manual)
))
colnames(metrics_val_total) <- c("threshold", "sensitivity", "auc", "specificity", "accuracy")
rownames(metrics_val_total) <- str_row_names
```

```{r}
# Sort by best sensitivity (true positive rate)
(metrics_val_total %>% arrange(desc(sensitivity)))[, c("sensitivity", "specificity", "accuracy", "threshold")]
```



```{r}
metrics_test_full_std <- evaluate_test_set(model = mod_full_std, df_test = df_test, threshold = metrics_val_full_std[["mean_threshold"]])
metrics_test_full_br <- evaluate_test_set(mod_full_br, df_test = df_test, threshold = metrics_val_full_br[["mean_threshold"]])
metrics_test_manual2 <- evaluate_test_set(mod_manual2, df_test = df_test, threshold = metrics_val_manual2[["mean_threshold"]])
metrics_test_stepwise_aic <- evaluate_test_set(mod_step_aic, df_test = df_test, threshold = metrics_val_stepwise_aic[["mean_threshold"]])
metrics_test_stepwise_bic <- evaluate_test_set(mod_step_bic, df_test = df_test, threshold = metrics_val_stepwise_bic[["mean_threshold"]])
metrics_test_lasso <- evaluate_test_set(mod_lasso, df_test = df_test, threshold = metrics_val_lasso[["mean_threshold"]])
metrics_test_lda <- evaluate_test_set(mod_lda, df_test = df_test, threshold = metrics_val_lda[["mean_threshold"]])
metrics_test_qda <- evaluate_test_set(mod_qda, df_test = df_test, threshold = metrics_val_qda[["mean_threshold"]])
metrics_test_nb <- evaluate_test_set(mod_nb, df_test = df_test, threshold = metrics_val_nb[["mean_threshold"]])
metrics_test_knn <- evaluate_test_set(mod_knn, df_test = df_test, threshold = metrics_val_knn[["mean_threshold"]])
metrics_test_gam_bic <- evaluate_test_set(mod_gam_bic, df_test = df_test, threshold = metrics_val_gam_bic[["mean_threshold"]])
metrics_test_gam_manual <- evaluate_test_set(mod_gam_manual, df_test = df_test, threshold = metrics_val_gam_manual[["mean_threshold"]])
```

```{r}
metrics_test_total <- as.data.frame(rbind(
  unlist(metrics_test_full_std),
  unlist(metrics_test_full_br),
  unlist(metrics_test_manual2),
  unlist(metrics_test_stepwise_aic),
  unlist(metrics_test_stepwise_bic),
  unlist(metrics_test_lasso),
  unlist(metrics_test_lda),
  unlist(metrics_test_qda),
  unlist(metrics_test_nb),
  unlist(metrics_test_knn),
  unlist(metrics_test_gam_bic),
  unlist(metrics_test_gam_manual)
))
colnames(metrics_test_total) <- c("sensitivity", "specificity", "accuracy")
rownames(metrics_test_total) <- str_row_names
```

```{r}
# Sort by best sensitivity (true positive rate)
(metrics_test_total %>% arrange(desc(sensitivity)))[, c("sensitivity", "specificity", "accuracy")]
```