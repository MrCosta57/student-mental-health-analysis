---
title: "Mind Matters: A Data-driven Examination of College Students' Mental Health"
author: "Giovanni Costa - 880892"
date: "AY 2023/24"
output:
  pdf_document:
    toc: true
    toc_depth: '2'
  html_document:
    toc: true
    number_sections: true
    toc_depth: 2
    toc_float:
      smooth_scroll: false
    fig_caption: true
    theme: flatly
    highlight: pygments
    css: assets/css/styles.css
geometry: left=1cm,right=2cm,top=1cm,bottom=1cm
subtitle: Statistical Inference and Learning project
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align = "center")
```


# Introduction
This project aims to delve into the pressing issue of mental health among college students, a topic of significant importance in today's fast-paced, high-stress academic environment. The analysis seeks to provide a comprehensive understanding of the mental health landscape, by utilizing one dataset coming from a survey, with a particular focus on psychiatric symptoms and other associated health problems.

The dataset, that can be retrieved <a href="https://psycharchives.org/en/item/5d25cf8a-7910-4579-9e39-688c6bbec43a">here</a>, contains 27014 objects with 16 features. The first six information are categorical and related to the general individual condition, instead, the others are numeric variables associated with the mental health of the students, where the higher score indicates more serious symptoms. Additional details regarding the data scales, the data origin, and the retrieving methodology are not provided by the dataset's website.<br/>
A specific description of these features is provided below:

1. **gender**: 0=Female, 1=Male
2. **whether_only_child**: 0=No, 1=Yes
3. **birth_place**: 0=Countryside, 1=Town, 2=SmallCity, 3=MediumToLargeCities
4. **family_economic_status**: 0=ExtremelyPoor, 1=Poor, 2=Average, 3=Good, 4=Rich
5. **major**: 0=Liberal, 1=Science, 2=Art
6. **grade**: 0=Postgraduate, 1=UndergraduateGradeFive, 2=Junior, 3=Sophomore, 4=Freshman, 5=Senior
7. **psychiatric_symptoms**: range from 4 to 16
8. **suicide**: range from 4 to 16
9. **dependence**: range from 4 to 16
10. **impulsivity**: range from 4 to 16
11. **compulsion**: range from 4 to 16
12. **sleeping_disturbance**: range from 4 to 16
13. **internet_addiction**: range from 5 to 20
14. **hostile_aggression**: range from 4 to 16
15. **self_injury_behaviors:** range from 4 to 16
16. **eating_problems**: range from 4 to 16

The purposes of this analysis are inspecting the possible hidden patterns present in the data and predicting the risk of observing a suicide, modeled for convenience as a binary variable. 

# Prerequirements
```{r results='FALSE', message=FALSE, warning=FALSE}
# Configure the environment
# options(digits = 4, scipen = 999)
rm(list = ls())
# Set seed for reproducibility
seed <- 123
num_k_fold <- 5
set.seed(seed)

# Load all the packages and install them if they are not present
requirements <- c(
  "summarytools", "MASS", "effects", "pROC", "mgcv",
  "glmnetUtils", "e1071", "class", "car", "brglm2",
  "gridExtra", "grid", "stringr", "ggplot2", "reshape2", "scales", "comprehenr",
  "dplyr", "ggcorrplot"
)
for (library_name in requirements) {
  if (!require(library_name, character.only = TRUE)) {
    install.packages(library_name, repos = "https://cloud.r-project.org")
    library(library_name, character.only = TRUE)
  }
}
# Import user defined functions
source("src/utils.R")
source("src/plotting.R")
# library(styler)
# style_file("analysis_report.Rmd")
```


# First overview

In this section, some operations are performed to understand the dataset structure and identify any potential issues that need to be addressed. Also, variables are transformed into the appropriate format.

```{r}
# Read the dataset
df <- read.csv("data/mental_health_data.csv")
# Remove the column representing the row index
df <- df[, -1]

# Rename existing columns
colnames(df)[colnames(df) == "whether.only.child"] <- "whether_only_child"
colnames(df)[colnames(df) == "birth.place"] <- "birth_place"
colnames(df)[colnames(df) == "family.economic.status"] <- "family_economic_status"
colnames(df)[colnames(df) == "psychiatric.symptoms"] <- "psychiatric_symptoms"
colnames(df)[colnames(df) == "sleeping.disturbance"] <- "sleeping_disturbance"
colnames(df)[colnames(df) == "internet.addiction"] <- "internet_addiction"
colnames(df)[colnames(df) == "hostile.aggression"] <- "hostile_aggression"
colnames(df)[colnames(df) == "self.injury.behaviors"] <- "self_injury_behaviors"
colnames(df)[colnames(df) == "eating.problems"] <- "eating_problems"

# Transforming variables into factors
df$gender <- factor(df$gender,
  levels = c(0, 1),
  labels = c("Female", "Male")
)
df$whether_only_child <- factor(df$whether_only_child,
  levels = c(0, 1),
  labels = c("No", "Yes")
)
df$birth_place <- factor(
  df$birth_place,
  levels = 0:3,
  labels = c("Countryside", "Town", "SmallCity", "MediumToLargeCities")
)
df$family_economic_status <- factor(
  df$family_economic_status,
  levels = 0:4,
  labels = c("ExtremelyPoor", "Poor", "Average", "Good", "Rich")
)
df$major <- factor(df$major,
  levels = 0:2,
  labels = c("Liberal", "Science", "Art")
)
df$grade <- factor(
  df$grade,
  levels = 0:5,
  labels = c(
    "Postgraduate",
    "UndergraduateGradeFive",
    "Junior",
    "Sophomore",
    "Freshman",
    "Senior"
  )
)
general_cols <- c(
  "gender", "whether_only_child", "birth_place",
  "family_economic_status", "major", "grade"
)
symptoms_cols <- c(
  "psychiatric_symptoms", "dependence", "impulsivity", "compulsion", "sleeping_disturbance",
  "internet_addiction", "hostile_aggression", "self_injury_behaviors", "eating_problems"
)
```

```{r}
print_summary_custom(df, method = "pander")
```

```{r}
# "Dataset dimensions:"
dim(df)
# "N. of missing values:"
sum(is.na(df))
# "Example of some objects:"
head(df, 3)
```

From the above table, it can be seen that all the columns have the expected format now. Also can be observed that there are no missing values.

Talking about the summary of the variables, different aspects can be observed:

- The majority of the students are female (67.5\%)
- A lot of people have at least one brother or sister (72.8\%)
- Fewer students in the dataset are from medium to large cities (10.2\%)
- The number of students with average family economic status is the highest (66.8\%), instead the number of students with rich family it quite low (0.4\%)
- The most common major is liberal (58.5\%)
- Only 0.3\% of the people are undergraduates with grade five, instead the other grades are quite equally distributed
- The mean value of the variable Internet addiction is near 10 (scale [5, 20]) while the mean for sleeping disturbance, impulsivity, compulsion, and dependence are around 7 (scale [4, 16]). These symptom scores are a bit higher with respect to the other variables values
- No peoples with self-injury behaviors has symptoms of value 15 and they have the lowest average score (4.82), suggesting it might be the least prevalent issue
- In general students with very high symptoms are fortunately the minority among the observed data sample, so the numeric distributions are right skewed.


As anticipated, in this analysis, it's decided to transform the variable *suicide* into a binary feature, considering the current problem as a classification task.<br/>
In this sense, the values of *suicide* lower or equal to 9 take the value *FALSE*, instead the other takes the value *TRUE*. The choice of this threshold is thought reasonable because the purpose of this analysis is to consider the higher severity of suicide but at the same time don't underestimate a real risk even if the value is not too big.


```{r}
print(summary(df$suicide))
threshold_suicide <- 9
numeric_suicide <- df$suicide
factor_suicide <- as.factor(df$suicide > threshold_suicide)
df$suicide <- factor_suicide

# Number of suicide
print(table(df$suicide))
# Percentage of suicide
print(table(df$suicide) / nrow(df) * 100)
```

Imposing this threshold for the response variable, the classes appear quite unbalanced: indeed the proportion of negative suicide in the dataset is `r table(df$suicide)[0] / nrow(df) * 100`\% and the proportion of positive suicide is `r table(df$suicide)[0] / nrow(df) * 100`\%.
Furthermore, in the model development section, some considerations on that will be stated.


# Exploratory Data Analysis
After a first overview of the data, it's needed to inspect some possible relations and hidden patterns between the variables. Some hypothesized interaction terms are also checked.

## Frequencies concerning response variable

### General information columns
```{r fig.height=4, fig.width=8, message=FALSE, warning=FALSE, echo=FALSE}
for (col in general_cols) {
  plot_freq_by_category(df, col, "suicide")
}
```

Looking at the bar plots referring to the general information about the students, it seems that there are no strange relationships between the categories and the values of the frequencies for the variable *suicide*: in particular the higher number of suicides for a categorical level is usually related with the higher number of person in that level.
Just for the grade^[https://uniexperts.com/en/news/freshman-sophomore-junior-senior-an-explanation/] *Freshmen* the number of suicides is higher than the other levels, and in particular, there are fewer suicides for older grading (e.g. postgraduate and undergraduate). Also, the *grade* variable is possibly related to some age information that in the dataframe is not present and it could suggest that younger people are less prone to suicide ideal. 


```{r fig.height=4, fig.width=8, message=FALSE, warning=FALSE, echo=FALSE}
for (col in general_cols) {
  plot_proportion_by_category(df, col, "suicide")
}
```

The plots above regarding the proportions of students for each category divided by the response values instead highlight interesting patterns: indeed it's more clear to understand the relationships considering ratios and not the frequencies, also because the y scale is different.

There are a few visible things in these visualizations:

- The relative difference in suicide frequency considering the gender seems not so significant based on these data, instead, the gap is more noticeable compared with *whether only child* variable
- It seems that the percentage of suicide in medium to large cities is higher.
- Being in a rich family leads to a higher proportion of suicide based on this dataset, even more, that being in an extremely poor family's economic condition
- Attempting the art major seems to have a higher risk compared with liberal and science courses
- As observed before, the percentage of positive values in the response variable is higher for the freshman category


### Symptoms information columns
```{r fig.height=4, fig.width=8, message=FALSE, warning=FALSE, echo=FALSE}
for (col in symptoms_cols) {
  plot_freq_by_category(df, col, "suicide")
}
```

The histograms of the symptoms features show a slightly different behavior compared with the previous categorical predictors' plots. As observed in the beginning, the number of high-severity symptoms in general is low but the frequency of suicide gets bigger when these symptoms increase.

```{r fig.height=4, fig.width=8, message=FALSE, warning=FALSE, echo=FALSE}
for (col in symptoms_cols) {
  plot_proportion_by_category(df, col, "suicide")
}
```

Checking the proportion histograms divided by the response variable what was stated previously becomes more evident: now it can be seen a noticeable increasing trend in the suicide ratios when the symptoms severity increases and in particular the percentage is higher for the variables psychiatric symptoms, hostile aggression, self-injury behaviors and eating problems, even if the frequency of the higher symptoms for them is quite low (except for impulsivity and compulsion variables)


## Numeric variables
```{r fig.height=4, fig.width=8, message=FALSE, warning=FALSE, echo=FALSE}
corr_matrix <- round(cor(df[, symptoms_cols]), 2)
ggcorrplot(corr_matrix,
  hc.order = TRUE,
  type = "lower",
  lab = TRUE,
  lab_size = 3,
  colors = c("#6D9EC1", "white", "#E46726"),
  title = "Pearson correlation",
  ggtheme = theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))
)
```

The heatmap of the correlation matrix shows that all the variables are quite highly positively correlated with each other. By the plot can be seen that the most correlated variables are *impulsivity* and *compulsion* (0.72), *dependence* and *impulsivity* (0.71), *dependence* and *compulsion* (0.68). Also *self-injury behaviors* and *eating problems* seem to be correlated in a significant way (0.66).


Inspecting the correlation matrix, it seems that some variables are quite high positive correlated. however this information is not fully considered because numeric variables have all positive variables and they are almost all in the same range.


## Interaction terms
Just for a better understanding of the possible interaction between the variables, some scatter plots are drawn to check if some evident patterns can be useful for model development. For this purpose, the response variable is considered as a numeric variable and a line is fitted to highlight the linear relation between variables divided by categories.

In particular, the following interactions are considered:

- **Psychiatric symptoms and gender**: It's important to inspect these variables because psychiatric conditions often manifest differently across genders, potentially leading to gender-specific patterns in symptoms and treatment outcomes.

- **Eating problems and family economic status**: Economic factors can affect access to resources and support for managing eating problems, influencing the severity and consequences of these issues in different socioeconomic groups.

- **Sleeping disturbance and major**: Different majors may have varying stress levels and workload demands, which can interact with sleep disturbances to affect academic performance and mental health uniquely in each field of study.

- **Hostile aggression and birth place**: Cultural norms around aggression and conflict resolution can vary significantly by location, impacting how hostile aggression influences social relationships and legal issues in different birthplaces.

```{r}
df$suicide <- numeric_suicide
jitter_val <- 1
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
ggplot(df, aes(x = jitter(psychiatric_symptoms, jitter_val), y = jitter(suicide, jitter_val), color = gender)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(group = gender)) +
  labs(
    title = "Suicide vs psychiatric symptoms by gender interaction",
    x = "Psychiatric symptoms",
    y = "Suicide"
  )
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
ggplot(df, aes(x = jitter(eating_problems, jitter_val), y = jitter(suicide, jitter_val), color = family_economic_status)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(group = family_economic_status)) +
  labs(
    title = "Suicide vs eating problems by family economic status interaction",
    x = "Eating problems",
    y = "Suicide"
  )
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
ggplot(df, aes(x = jitter(sleeping_disturbance, jitter_val), y = jitter(suicide, jitter_val), color = major)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(group = major)) +
  labs(
    title = "Suicide vs sleeping disturbance by major interaction",
    x = "Sleeping disturbance",
    y = "Suicide"
  )
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
ggplot(df, aes(x = jitter(hostile_aggression, jitter_val), y = jitter(suicide, jitter_val), color = birth_place)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, aes(group = birth_place)) +
  labs(
    title = "Suicide vs hostile_aggression by birth_place interaction",
    x = "Birth place",
    y = "Suicide"
  )
```
```{r}
df$suicide <- factor_suicide
```

For these data, it seems that just the interaction between *eating problems and family economic status* and *sleeping disturbance and major* is relevant.

# Models development
In a statistical analysis, splitting data into training and testing sets is crucial for model development. This process allows building the model using the training set and then assessing its quality on the unseen data at the very late stage using the test set. The hyperparameter tuning, where required, and the performance metrics are instead achieved by cross-validation on the training partition.<br/>
This is needed to check if the model generalizes well to new data and provides an unbiased estimate of its predictive accuracy.

The reference level for the categorical predictors in a full model is a female student with brothers/sisters, from the countryside with an extremely poor family, liberal major, and postgraduate.

```{r}
idx <- sample(nrow(df), 0.8 * nrow(df))
df_train <- df[idx, ]
df_test <- df[-idx, ]

print(paste0("Train size: ", nrow(df_train)))
print(paste0("Test size: ", nrow(df_test)))

# Reference levels
for (i in general_cols) {
  print(paste0(i, ": ", levels(df[, i])[1], sep = ""))
}
# Class frequency in the training set
print(table(df_train$suicide))
# Class proportion in the training set
print(table(df_train$suicide) / nrow(df_train) * 100)

# Class frequency in the test set
print(table(df_test$suicide))
# Class proportion in the test set
print(table(df_test$suicide) / nrow(df_test) * 100)
```


## Logistic regression
Logistic regression is used for modeling directly the probability that the response variable belongs to a particular class.
The coefficients of the generalized linear models in this case represent the log-odds change of the outcome (suicide) for a one-unit increase in the predictor variable, holding all other variables constant. A positive coefficient indicates an increase in the log-odds of suicide, while a negative coefficient means a decrease.

### Full model
```{r}
mod_full_std <- glm(suicide ~ ., data = df_train, family = "binomial")
summary(mod_full_std)
```

Examining the model that includes all the variables, several predictors appear to be strongly significant. Notably, the intercept, which corresponds to the reference level and numeric predictors with coefficients of 0, has a log-odds value of `r mod_full_std$coefficients[1]`. When converted, this log-odds value represents a probability of `r exp(mod_full_std$coefficients[1])/(1-exp(mod_full_std$coefficients[1]))` for the response variable $\mathcal{P}(suicide)=\frac{e^{\beta_0}}{1+e^{\beta_0}}$.


In particular, the following model interpretation is considered:

- Being male decreases the log-odds of suicide compared to being female. This suggests that males are less likely to report suicidal tendencies than females in this dataset
- Students from towns, small cities, and medium to large cities have higher log-odds of suicide compared to students from rural areas. This indicates that urbanization might be associated with higher suicidal tendencies.
- Being in a family with poor, average, or good economic status leads to a large decrease in the log-odds compared with the base level, instead being in a rich family seems to be not significant in this model 
- Freshmen grade has the highest value for the coefficient and it's strongly significant.
- All the symptom predictors are statistically significant or strongly significant except for *eating_problems* in this model. Higher levels of psychiatric symptoms, impulsivity, compulsion, sleeping disturbances, internet addiction, hostile aggression, and self-injury behaviors are associated with increased log-odds of suicide. Dependence, however, shows a negative relationship, suggesting that higher dependence decreases the log-odds of suicide for these data.
- The levels in whether only child and in major have quite high p-values for the significance test and they seem not influential

```{r}
vif(mod_full_std)
cook_dist <- cooks.distance(mod_full_std)
# Plot using ggplot2
ggplot(
  data.frame(Index = 1:length(cook_dist), Cook = cook_dist),
  aes(x = Index, y = Cook)
) +
  geom_point() +
  geom_line() +
  labs(x = "Observation Index", y = "Cook's Distance") +
  ggtitle("Cook's Distance for Each Observation") +
  theme_minimal()
cook_dist <- NULL
```
The model doesn't show significant signs of multicollinearity or influential points

The above interpretation can be also visualized by checking the effect plots, paying attention to the fact that now the y-axis is represented in the response scale (not the log-odds one)

```{r fig.height=23, fig.width=15}
plts <- list()
for (i in general_cols) {
  p <- plot(effect(i, mod_full_std), ylab = "Probability of Suicide", rescale.axis = FALSE)
  plts[[i]] <- p
}
grid.arrange(grobs = plts, top = textGrob("General features effect plots", gp = gpar(fontsize = 20, font = 3)))
plts <- NULL
```
```{r fig.height=23, fig.width=18}
plts <- list()
for (i in symptoms_cols) {
  p <- plot(effect(i, mod_full_std), ylab = "Probability of Suicide", rescale.axis = FALSE)
  plts[[i]] <- p
}
grid.arrange(grobs = plts, top = textGrob("Symptoms features effect plots", gp = gpar(fontsize = 20, font = 3)))
plts <- NULL
```


Models fitted using the standard Maximum likelihood method where the proportion of classes in the response variable is unbalanced could not be optimal.
Specifically, to overcome this problem, bias-corrected maximum likelihood^[Kosmidis, I., Kenne Pagui, E.C. & Sartori, N. Mean and median bias reduction in generalized linear models. Stat Comput 30, 43–59 (2020)] technique is utilized for estimating the coefficients of the logistic regression models robustly.

```{r}
mod_zero_br <- glm(suicide ~ 1, data = df_train, family = "binomial", method = "brglmFit")
mod_full_br <- glm(suicide ~ ., data = df_train, family = "binomial", method = "brglmFit")
```
```{r}
tab <- cbind(coef(mod_full_std), coef(mod_full_br), abs(coef(mod_full_std) - coef(mod_full_br)))
colnames(tab) <- c("Vanilla Model", "Full Model", "Abs diff.")
tab
tab <- NULL
```
Models present slight differences in the coefficient estimations in this case, but due to the better theoretical properties of the bias reduction methods for logistic regression, this one is utilized for the analysis.


### Manual selection

Given the available information, an attempt to develop a model manually is proposed.
Specifically, the first model contains some significant variables present in the full model, the previously checked interaction terms that seem influential and their main effects, impulsivity feature that in the corr matrix seems the most correlated variable. The second model instead contains some possible variables that appear more discriminative

```{r}
mod_manual1_interaction <- glm(
  suicide ~ impulsivity + self_injury_behaviors + family_economic_status +
    eating_problems + major + sleeping_disturbance +
    family_economic_status:eating_problems + major:sleeping_disturbance,
  data = df_train,
  family = "binomial",
  method = "brglmFit"
)
summary(mod_manual1_interaction)

mod_manual2 <- glm(
  suicide ~ gender + grade + birth_place + impulsivity + self_injury_behaviors +
    hostile_aggression + psychiatric_symptoms,
  data = df_train,
  family = "binomial",
  method = "brglmFit"
)
summary(mod_manual2)

print(BIC(mod_full_br, mod_manual1_interaction, mod_manual2))
```

Including the interaction terms and the variables considered in the scatter plots seems to lead to no benefit to the model because they appear not statistically significant based on these data. Also the BIC score, which penalizes more complex models, is higher compared with the GLM with all the predictors. The second manual has all significant predictors, except for some levels in the variable *grade*. The information criteria is higher also in this case to the full model.


### Stepwise regression
Bidirectional Stepwise Regression, is a variable selection method that combines both forward and backward stepwise regression approaches. It iteratively adds and removes predictors based on a predefined information criterion until no further improvements are observed. This method provides a balance between computational efficiency and model selection performance, making it suitable for situations where automated variable selection is desired while controlling for overfitting.

```{r}
# AIC stepwise regression
mod_step_aic <- stepAIC(mod_zero_br, direction = "both", scope = list(upper = formula(mod_full_br)), trace = 0)
summary(mod_step_aic)
```

The logistic regression model identifies several significant predictors of suicide among students. Psychiatric symptoms, sleeping disturbance, self-injury behaviors, compulsion, grade (sophomores, freshmen and senior), hostile aggression, birth places, gender, family economic status, dependence and impulsivity show statistically significant associations with the likelihood of suicide.<br/>
Notably, the predictors associated with a lower likelihood of suicide with respect to the reference level are gender, the other family economic conditions (except *Rich*) and dependence.<br/>
The model seems to have a good explainable power as indicated the the slight reduction of the AIC score with respect to the full model, however, the predictors are still a bit (12 features selected vs 15 total features).

It's possible to try stepwise regression using BIC penalty to select a more parsimonious model.

```{r}
# BIC stepwise regression
mod_step_bic <- stepAIC(mod_zero_br, direction = "both", scope = list(upper = formula(mod_full_br)), trace = 0, k = log(nrow(df_train)))
summary(mod_step_bic)
```

Both models consistently identify psychiatric symptoms, sleeping disturbance, self-injury behaviors, compulsion and hostile aggression as significant predictors and here there's strong significant evidence on all. The signs of the estimated coefficients are the same for both.<br/>
The choice between the models depends on the preference for model complexity versus simplicity and the specific goals of the analysis.

For completeness, effect plots from the stepwise regression model with AIC are provided:
```{r fig.height=18, fig.width=15}
plts <- list()
for (i in extract_predictors_in_vec(mod_step_aic$formula, general_cols)) {
  p <- plot(effect(i, mod_step_aic), ylab = "Probability of Suicide", rescale.axis = FALSE)
  plts[[i]] <- p
}
grid.arrange(grobs = plts, top = textGrob("General features effect plots", gp = gpar(fontsize = 20, font = 3)))
plts <- NULL
```

```{r fig.height=23, fig.width=18}
plts <- list()
for (i in extract_predictors_in_vec(mod_step_aic$formula, symptoms_cols)) {
  p <- plot(effect(i, mod_step_aic), ylab = "Probability of Suicide", rescale.axis = FALSE)
  plts[[i]] <- p
}
grid.arrange(grobs = plts, top = textGrob("Symptoms features effect plots", gp = gpar(fontsize = 20, font = 3)))
plts <- NULL
```


### Lasso shrinkage method
Lasso technique introduces a penalty term to the standard regression objective function, encouraging sparsity by shrinking the coefficients of less important predictors toward zero. This regularization performs a feature selection, making it particularly useful when dealing with high-dimensional datasets with potentially correlated predictors.

Notice that the implementation of Lasso in `glmnetUtils` R library^[https://cran.r-project.org/web/packages/glmnetUtils/vignettes/intro.html] deliberately avoids the usual treatment of factors with a reference level “absorbed” by the intercept. The model type, in this case, it's more interpretable because otherwise, the lasso shrinkage could cancel the differences in the factor levels of the predictors.


```{r}
mod_lasso <- cv.glmnet(suicide ~ ., data = df_train, family = "binomial", method = "brglmFit", alpha = 1, nfolds = num_k_fold)
beta_1se <- coef(mod_lasso, s = mod_lasso$lambda.1se)
beta_1se_idx <- which(beta_1se[, 1] != 0)
print(cbind(beta_1se_idx, beta_1se[beta_1se_idx])[, 2])
print(sum(beta_1se[, 1] != 0))
```

The fitted logistic regression model suggests several factors, all related to symptom variables and they contribute all positively to a higher risk of suicide.

If compared to the coefficients selected from stepwise regression, the magnitude of the common coefficients is usually smaller, indicating that the lasso model tends to shrink the coefficients towards zero as expected. In addition, BIC and Lasso models select the same set of predictors, even if the latter includes also *impulsivity* predictor.


The below plots show the coefficients shrinkage with respect to the log lambda value in the first, and the behavior of the binomial deviance with respect to this hyperparameter in the latter. The dotted line represents the log lambda value that minimizes the cross-validated error, while the dashed line represents the log lambda value that is within one standard error of the minimum.<br/>
It's decided to take the model with the lambda value plus one standard error because it's more parsimonious and the difference in the binomial deviance is not so high concerning the minimum value.

```{r echo=FALSE}
plot(mod_lasso$glmnet.fit, xvar = "lambda")
abline(v = log(mod_lasso$lambda.min), lty = "dotted", col = "darkorange")
abline(v = log(mod_lasso$lambda.1se), lty = "dashed", col = "steelblue")
legend("topright",
  legend = c("lambda.min", "lambda.1se"),
  col = c("darkorange", "steelblue"),
  lty = c("dotted", "dashed"),
  cex = 0.8
)

plot(mod_lasso)
abline(v = log(mod_lasso$lambda.min), lty = "dotted", col = "darkorange")
abline(v = log(mod_lasso$lambda.1se), lty = "dashed", col = "steelblue")
legend("topright",
  legend = c("lambda.min", "lambda.1se"),
  col = c("darkorange", "steelblue"),
  lty = c("dotted", "dashed"),
  cex = 0.8
)
```


## Generative models
After modeling directly the response variable by the logistic regression based models, it's possible to consider generative models that estimate the class-specific densities and then use Bayes' theorem to derive the posterior probabilities of the classes. These types of models are more stable and robust when there's a substantial separation between the classes as probably is in this case.

To enhance interpretability for all the generative models, the preditors selected by stepwise regression with BIC are used in the model fitting.

### Linear Discriminant Analysis (LDA)
The LDA model assumes that the predictors are normally distributed and that the covariance matrices are equal across classes.
```{r}
lda_formula <- mod_step_bic$formula
mod_lda <- lda(lda_formula, data = df_train)
mod_lda
```
The printed output of LDA includes the a-priori probabilities of suicide and the group means.<br/>
The proportion of training observation that belongs to positive suicide variables is quite low. As previously observed different times, when the response variable is true and the predictors are the symptom variables, their mean is higher compared with the case in which the response variable is false.


### Quadratic Discriminant Analysis (QDA)
The QDA model generalizes the LDA assuming different covariance matrices for each class, so the flexibility is increased but the number of parameters to estimate is much larger

```{r}
qda_formula <- mod_step_bic$formula
mod_qda <- qda(qda_formula, data = df_train)
mod_qda
```

### Naive Bayes
Naive Bayes instead assumes independence between the predictors to estimate the class-specific densities: this assumption usually introduces bias but reduces the variance component of the error. The standard naive Bayes classifier for estimating univariate class densities of quantitative predictors assumes Gaussian distribution (likelihood component in the Bayes' formula)

```{r}
nb_formula <- mod_step_bic$formula
mod_nb <- naiveBayes(nb_formula, data = df_train)
mod_nb
```
The output of Naive Bayes contains the estimated a-priori probabilities and a table containing the mean and standard deviation of the numeric variables, conditioned by the response.


## Other models

### Generalized Additive Model (GAM)
GAMs extend traditional linear models by allowing for non-linear relationships between predictors and the response variable through the use of smooth functions. This flexibility enables the model to capture complex patterns in the data without assuming a specific parametric form.

In particular, the function `gam` of the R package `mgcv` by default includes in the nonlinear model the smooth terms using thin plate regression splines^[Thin plate regression splines are described in detail in Wood (2003), *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, 65(1), 95-114.], where the degree of smoothness is estimated as part of the fitting procedure.

```{r}
# extract only the predictors from the formula and model the numeric one as smooth terms
gam_formula_bic <- formula(paste(
  "suicide~",
  paste(extract_predictors_in_vec(mod_step_bic$formula, general_cols), collapse = " + "),
  " + ",
  paste(
    "s(",
    extract_predictors_in_vec(mod_step_bic$formula, symptoms_cols),
    ")",
    collapse = "+"
  )
))

mod_gam_bic <- gam(gam_formula_bic, family = "binomial", data = df_train)
summary(mod_gam_bic)
```

All five smooth terms are considered statistically significant according to the p-value and the model explains 36.8% of the deviance. In addition, the estimated effective degrees of freedom (edf) propose complex fits for all the predictors, suggesting the presence of nonlinear effects in the data. Notice also that higher edf values suggest more flexibility in capturing non-linear effects.

```{r fig.height=8, fig.width=12}
par(mfrow = c(1, 2))
plot(mod_gam_bic, shade = TRUE, shade.col = "lightblue")
par(mfrow = c(1, 1))
```

It's decided to try including some additional predictors considered significant in other models.

```{r}
mod_gam_manual <- update(mod_gam_bic, . ~ . + grade + birth_place + s(impulsivity))
summary(mod_gam_manual)
```
The *grade* categorical predictor presents some levels not statistically significant also in this model, instead the other terms seem to have an influence. The manual GAM model explains now the 38\% of the deviance<br/>
The nonlinear effect of *impulsivity* appears not statistically significant so it's decided to remove it.

```{r}
mod_gam_manual <- update(mod_gam_manual, . ~ . - s(impulsivity))
summary(mod_gam_manual)
```
```{r}
AIC(mod_gam_bic, mod_gam_manual)
```
As can be observed the new manual-designed model seems to fit better the data due to the lower AIC score.


# Model comparison
The performance of the models is evaluated using different metrics but in this analysis, the sensitivity (true positive rate) is considered crucial. Indeed, this score refers to the model's ability to correctly identify individuals at risk of suicide, and missing a positive case (i.e., failing to predict a suicide risk when one exists) can have severe and life-threatening consequences.<br/>
The optimal threshold for the response variable prediction is determined by the point on the ROC curve that maximizes the Youden's J statistic, $max(sensitivity+specificity)$, which balances the true positive and false positive rates.<br/>
As anticipated in the previous section, the validation set is used to assess the model metrics to reduce the error variability introduced by the random splitting of the dataset, instead, the test set is utilized at the end to evaluate the final model performance. Note that the evaluations on the test set are performed using the models fitted on the whole training set: indeed validation set is no longer needed and all the possible data are utilized for training purposes.

```{r}
str_row_names <- c(
  "Full std model", "Full br model", "Manual model 2", "Stepwise regression with AIC",
  "Stepwise regression with BIC", "Lasso", "LDA", "QDA", "Naive Bayes", "GAM with BIC", "GAM manual"
)
# Summary of the information criteria when available
info_criteria <- as.data.frame(cbind(
  AIC(mod_full_std, mod_full_br, mod_manual2, mod_step_aic, mod_step_bic, mod_gam_bic, mod_gam_manual)[, 2],
  BIC(mod_full_std, mod_full_br, mod_manual2, mod_step_aic, mod_step_bic, mod_gam_bic, mod_gam_manual)[, 2]
))
colnames(info_criteria) <- c("AIC", "BIC")
rownames(info_criteria) <- str_row_names[c(1, 2, 3, 4, 5, 10, 11)]
```

```{r}
# Sort by AIC
(info_criteria %>% arrange(AIC))[, c("AIC", "BIC")]
```
```{r}
# Sort by BIC
(info_criteria %>% arrange(BIC))[, c("BIC", "AIC")]
```
The information criteria for the models containing all the predictors are very similar. The lower AIC score instead is from the manual-designed GAM, highlighting that this one has a good balance between the goodness of fitting and the complexity of the model. The model with the lower BIC score is instead the linear one selected by stepwise regression with BIC and it's the more parsimonious.

```{r message=FALSE, warning=FALSE}
metrics_val_full_std <- k_fold_cv(
  model_formula = mod_full_std$formula, model_func = stats::glm, data = df_train,
  n_fold = num_k_fold,
  family = "binomial"
)
metrics_val_full_br <- k_fold_cv(
  model_formula = mod_full_br$formula, model_func = stats::glm, data = df_train,
  n_fold = num_k_fold,
  family = "binomial", method = "brglmFit"
)
metrics_val_manual2 <- k_fold_cv(
  model_formula = mod_manual2$formula, model_func = stats::glm, data = df_train,
  n_fold = num_k_fold,
  family = "binomial", method = "brglmFit"
)
metrics_val_stepwise_aic <- k_fold_cv(
  model_formula = mod_step_aic$formula, model_func = stats::glm, data = df_train,
  n_fold = num_k_fold,
  family = "binomial", method = "brglmFit"
)
metrics_val_stepwise_bic <- k_fold_cv(
  model_formula = mod_step_bic$formula, model_func = stats::glm, data = df_train,
  n_fold = num_k_fold,
  family = "binomial", method = "brglmFit"
)
metrics_val_lasso <- k_fold_cv(
  model_formula = formula("suicide~."), model_func = glmnetUtils::glmnet, data = df_train,
  n_fold = num_k_fold,
  family = "binomial", method = "brglmFit", lambda = mod_lasso$lambda.1se, alpha = 1
)
metrics_val_lda <- k_fold_cv(
  model_formula = lda_formula, model_func = MASS::lda, data = df_train,
  n_fold = num_k_fold, is_generative = TRUE
)
metrics_val_qda <- k_fold_cv(
  model_formula = qda_formula, model_func = MASS::qda, data = df_train,
  n_fold = num_k_fold, is_generative = TRUE
)
metrics_val_nb <- k_fold_cv(
  model_formula = nb_formula, model_func = e1071::naiveBayes, data = df_train,
  n_fold = num_k_fold, is_nb = TRUE
)
metrics_val_gam_bic <- k_fold_cv(
  model_formula = mod_gam_bic$formula, model_func = mgcv::gam, data = df_train,
  n_fold = num_k_fold, family = "binomial"
)
metrics_val_gam_manual <- k_fold_cv(
  model_formula = mod_gam_manual$formula, model_func = mgcv::gam, data = df_train,
  n_fold = num_k_fold, family = "binomial"
)
```
```{r message=FALSE, warning=FALSE}
metrics_val_total <- as.data.frame(rbind(
  unlist(metrics_val_full_std),
  unlist(metrics_val_full_br),
  unlist(metrics_val_manual2),
  unlist(metrics_val_stepwise_aic),
  unlist(metrics_val_stepwise_bic),
  unlist(metrics_val_lasso),
  unlist(metrics_val_lda),
  unlist(metrics_val_qda),
  unlist(metrics_val_nb),
  unlist(metrics_val_gam_bic),
  unlist(metrics_val_gam_manual)
))
colnames(metrics_val_total) <- c("threshold", "sensitivity", "auc", "specificity", "accuracy")
rownames(metrics_val_total) <- str_row_names
```
```{r}
# Sort by best sensitivity (true positive rate)
(metrics_val_total %>% arrange(desc(sensitivity)))[, c("sensitivity", "auc", "specificity", "accuracy", "threshold")]
```
Considering the evaluation metrics, some considerations can be stated:

- The model selected with stepwise regression and AIC achieves the highest score
- The models containing all the predictions have a good prediction power based on these tests but have the disadvantage of being less interpretable due to the large number of coefficients. Manual GAM has a sensitivity score very near to the full models
- The generative models perform a little worse compared with the others but the also use less predictors
- In general, all the best probability thresholds selected for the binary predictions are quite low.


```{r}
metrics_test_full_std <- evaluate_test_set(model = mod_full_std, df_test = df_test, threshold = metrics_val_full_std[["mean_threshold"]])
metrics_test_full_br <- evaluate_test_set(mod_full_br, df_test = df_test, threshold = metrics_val_full_br[["mean_threshold"]])
metrics_test_manual2 <- evaluate_test_set(mod_manual2, df_test = df_test, threshold = metrics_val_manual2[["mean_threshold"]])
metrics_test_stepwise_aic <- evaluate_test_set(mod_step_aic, df_test = df_test, threshold = metrics_val_stepwise_aic[["mean_threshold"]])
metrics_test_stepwise_bic <- evaluate_test_set(mod_step_bic, df_test = df_test, threshold = metrics_val_stepwise_bic[["mean_threshold"]])
metrics_test_lasso <- evaluate_test_set(mod_lasso, df_test = df_test, threshold = metrics_val_lasso[["mean_threshold"]])
metrics_test_lda <- evaluate_test_set(mod_lda, df_test = df_test, threshold = metrics_val_lda[["mean_threshold"]], is_generative = TRUE)
metrics_test_qda <- evaluate_test_set(mod_qda, df_test = df_test, threshold = metrics_val_qda[["mean_threshold"]], is_generative = TRUE)
metrics_test_nb <- evaluate_test_set(mod_nb, df_test = df_test, threshold = metrics_val_nb[["mean_threshold"]], is_nb = TRUE)
metrics_test_gam_bic <- evaluate_test_set(mod_gam_bic, df_test = df_test, threshold = metrics_val_gam_bic[["mean_threshold"]])
metrics_test_gam_manual <- evaluate_test_set(mod_gam_manual, df_test = df_test, threshold = metrics_val_gam_manual[["mean_threshold"]])
```
```{r}
metrics_test_total <- as.data.frame(rbind(
  unlist(metrics_test_full_std),
  unlist(metrics_test_full_br),
  unlist(metrics_test_manual2),
  unlist(metrics_test_stepwise_aic),
  unlist(metrics_test_stepwise_bic),
  unlist(metrics_test_lasso),
  unlist(metrics_test_lda),
  unlist(metrics_test_qda),
  unlist(metrics_test_nb),
  unlist(metrics_test_gam_bic),
  unlist(metrics_test_gam_manual)
))
colnames(metrics_test_total) <- c("sensitivity", "specificity", "accuracy")
rownames(metrics_test_total) <- str_row_names
```
```{r}
# Sort by best sensitivity (true positive rate)
(metrics_test_total %>% arrange(desc(sensitivity)))[, c("sensitivity", "specificity", "accuracy")]
```

Considering instead the scores obtained on the test set, the situation changes a bit:

- Indeed the GAM model with BIC formula reaches a very high sensitivity score
- The model fitted on all the predictors reach almost a score similar to the previous one but maybe the overfits a bit on the data, considering these results
- Impressively Naive Bayes obtain a high sensitivity score: maybe for this dataset the assumption of the independence between variables are not so incorrect


# Conclusions
Summarizing, this analysis highlights various aspects of the data related to the student's mental health, even in the absence of more detailed information about the students and the source of the data.
Through exploratory data analysis, it was observed that students experiencing more severe mental symptoms are generally at a higher risk of suicidal ideation. The most significant factors influencing this risk are psychiatric symptoms and self-injurious behaviors.

Additionally, some specific information such as being freshmen, females, and those from extremely poor or wealthy family backgrounds, appear to have a notable impact on the outcome variable. 

In terms of modeling, a decision was made to use fewer predictors to enhance interpretability, which was considered crucial for this analysis.
This choice may have slightly compromised accuracy, but most models achieved high sensitivity scores.
The Generative Additive Model (GAM) emerged as the best compromise between validation and test scores. This model also indicated the presence of nonlinear patterns among the predictors, suggesting a complex interaction mainly between suicide probability and mental symptoms. If greater interpretability and less computational power are preferred, standard logistic regression or Naive Bayes classifier using a subset of predictors can be a viable alternative for this data.